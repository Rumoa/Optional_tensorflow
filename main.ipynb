{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"krkopt.data\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = [\"wkc\", \"wkr\", \"wrc\", \"wrr\", \"bkc\", \"bkr\", \"opt rank\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wkc</th>\n",
       "      <th>wkr</th>\n",
       "      <th>wrc</th>\n",
       "      <th>wrr</th>\n",
       "      <th>bkc</th>\n",
       "      <th>bkr</th>\n",
       "      <th>opt rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>b</td>\n",
       "      <td>3</td>\n",
       "      <td>c</td>\n",
       "      <td>2</td>\n",
       "      <td>draw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>c</td>\n",
       "      <td>1</td>\n",
       "      <td>c</td>\n",
       "      <td>2</td>\n",
       "      <td>draw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>c</td>\n",
       "      <td>1</td>\n",
       "      <td>d</td>\n",
       "      <td>1</td>\n",
       "      <td>draw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>c</td>\n",
       "      <td>1</td>\n",
       "      <td>d</td>\n",
       "      <td>2</td>\n",
       "      <td>draw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>c</td>\n",
       "      <td>2</td>\n",
       "      <td>c</td>\n",
       "      <td>1</td>\n",
       "      <td>draw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28051</th>\n",
       "      <td>b</td>\n",
       "      <td>1</td>\n",
       "      <td>g</td>\n",
       "      <td>7</td>\n",
       "      <td>e</td>\n",
       "      <td>5</td>\n",
       "      <td>sixteen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28052</th>\n",
       "      <td>b</td>\n",
       "      <td>1</td>\n",
       "      <td>g</td>\n",
       "      <td>7</td>\n",
       "      <td>e</td>\n",
       "      <td>6</td>\n",
       "      <td>sixteen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28053</th>\n",
       "      <td>b</td>\n",
       "      <td>1</td>\n",
       "      <td>g</td>\n",
       "      <td>7</td>\n",
       "      <td>e</td>\n",
       "      <td>7</td>\n",
       "      <td>sixteen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28054</th>\n",
       "      <td>b</td>\n",
       "      <td>1</td>\n",
       "      <td>g</td>\n",
       "      <td>7</td>\n",
       "      <td>f</td>\n",
       "      <td>5</td>\n",
       "      <td>sixteen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28055</th>\n",
       "      <td>b</td>\n",
       "      <td>1</td>\n",
       "      <td>g</td>\n",
       "      <td>7</td>\n",
       "      <td>g</td>\n",
       "      <td>5</td>\n",
       "      <td>sixteen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28056 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      wkc  wkr wrc  wrr bkc  bkr opt rank\n",
       "0       a    1   b    3   c    2     draw\n",
       "1       a    1   c    1   c    2     draw\n",
       "2       a    1   c    1   d    1     draw\n",
       "3       a    1   c    1   d    2     draw\n",
       "4       a    1   c    2   c    1     draw\n",
       "...    ..  ...  ..  ...  ..  ...      ...\n",
       "28051   b    1   g    7   e    5  sixteen\n",
       "28052   b    1   g    7   e    6  sixteen\n",
       "28053   b    1   g    7   e    7  sixteen\n",
       "28054   b    1   g    7   f    5  sixteen\n",
       "28055   b    1   g    7   g    5  sixteen\n",
       "\n",
       "[28056 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, 0:6]\n",
    "y = data['opt rank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wkc</th>\n",
       "      <th>wkr</th>\n",
       "      <th>wrc</th>\n",
       "      <th>wrr</th>\n",
       "      <th>bkc</th>\n",
       "      <th>bkr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>b</td>\n",
       "      <td>3</td>\n",
       "      <td>c</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>c</td>\n",
       "      <td>1</td>\n",
       "      <td>c</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>c</td>\n",
       "      <td>1</td>\n",
       "      <td>d</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>c</td>\n",
       "      <td>1</td>\n",
       "      <td>d</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>c</td>\n",
       "      <td>2</td>\n",
       "      <td>c</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28051</th>\n",
       "      <td>b</td>\n",
       "      <td>1</td>\n",
       "      <td>g</td>\n",
       "      <td>7</td>\n",
       "      <td>e</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28052</th>\n",
       "      <td>b</td>\n",
       "      <td>1</td>\n",
       "      <td>g</td>\n",
       "      <td>7</td>\n",
       "      <td>e</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28053</th>\n",
       "      <td>b</td>\n",
       "      <td>1</td>\n",
       "      <td>g</td>\n",
       "      <td>7</td>\n",
       "      <td>e</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28054</th>\n",
       "      <td>b</td>\n",
       "      <td>1</td>\n",
       "      <td>g</td>\n",
       "      <td>7</td>\n",
       "      <td>f</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28055</th>\n",
       "      <td>b</td>\n",
       "      <td>1</td>\n",
       "      <td>g</td>\n",
       "      <td>7</td>\n",
       "      <td>g</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28056 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      wkc  wkr wrc  wrr bkc  bkr\n",
       "0       a    1   b    3   c    2\n",
       "1       a    1   c    1   c    2\n",
       "2       a    1   c    1   d    1\n",
       "3       a    1   c    1   d    2\n",
       "4       a    1   c    2   c    1\n",
       "...    ..  ...  ..  ...  ..  ...\n",
       "28051   b    1   g    7   e    5\n",
       "28052   b    1   g    7   e    6\n",
       "28053   b    1   g    7   e    7\n",
       "28054   b    1   g    7   f    5\n",
       "28055   b    1   g    7   g    5\n",
       "\n",
       "[28056 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"wkc\"]=X[\"wkc\"].astype('category')\n",
    "X[\"wrc\"]=X[\"wrc\"].astype('category')\n",
    "X[\"bkc\"]=X[\"bkc\"].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"wkc\"]=X[\"wkc\"].cat.codes\n",
    "X[\"wrc\"]=X[\"wrc\"].cat.codes\n",
    "X[\"bkc\"]=X[\"bkc\"].cat.codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wkc</th>\n",
       "      <th>wkr</th>\n",
       "      <th>wrc</th>\n",
       "      <th>wrr</th>\n",
       "      <th>bkc</th>\n",
       "      <th>bkr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28051</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28052</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28053</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28054</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28055</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28056 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       wkc  wkr  wrc  wrr  bkc  bkr\n",
       "0        0    1    1    3    2    2\n",
       "1        0    1    2    1    2    2\n",
       "2        0    1    2    1    3    1\n",
       "3        0    1    2    1    3    2\n",
       "4        0    1    2    2    2    1\n",
       "...    ...  ...  ...  ...  ...  ...\n",
       "28051    1    1    6    7    4    5\n",
       "28052    1    1    6    7    4    6\n",
       "28053    1    1    6    7    4    7\n",
       "28054    1    1    6    7    5    5\n",
       "28055    1    1    6    7    6    5\n",
       "\n",
       "[28056 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.astype('category')\n",
    "y = y.cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                   y, test_size=0.2,\n",
    "                                                   random_state = 1)\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "oversample = SMOTE()\n",
    "\n",
    "\n",
    "X_smote, y_smote = oversample.fit_resample(X, y)\n",
    "\n",
    "X_train_smote, X_test_smote, y_train_smote, y_test_smote = train_test_split(X_smote,\n",
    "                                                   y_smote, test_size=0.2,\n",
    "                                                   random_state = 1)\n",
    "\n",
    "\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test  = to_categorical(y_test)\n",
    "y_train_smote = to_categorical(y_train_smote)\n",
    "y_test_smote = to_categorical(y_test_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "scaler = StandardScaler().fit(X_train_smote)\n",
    "X_train_smote = scaler.transform(X_train_smote)\n",
    "X_test_smote = scaler.transform(X_test_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "\n",
    "checkpoint_filepath = '/tmp/checkpoint'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_my_model_multi( units_per_layer, input_s, output_s, activation_='relu'):\n",
    "    model = Sequential()\n",
    "    depth = len(units_per_layer)\n",
    "    model.add(Dense(units_per_layer[0], activation=activation_, input_shape=(input_s,)))\n",
    "    for i in range(1, depth):\n",
    "        model.add(Dense(units_per_layer[i], activation=activation_))\n",
    "    model.add(Dense(output_s, activation = 'softmax'))   \n",
    "    \n",
    "    return model\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=True)\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True,\n",
    "    verbose=True)\n",
    "\n",
    "\n",
    "#En este caso, usamos categorical_crossentropy como función de coste y \n",
    "#permitimos elegir otro batch para poder hacer uso de \n",
    "def compile_fit_multiclass(model, X_train, X_test, y_train, batch, epochs, verbose=0):\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch, verbose=verbose, validation_split=0.1, callbacks = [early_stopping, model_checkpoint])\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    predictions = model.predict(X_test)\n",
    "    return predictions\n",
    "\n",
    "def compute_metrics_multiclass(y_test, y_pred):\n",
    "    results=[]\n",
    "    results.append(precision_score(y_test, np.round(y_pred), average=\"micro\"))\n",
    "    results.append(recall_score(y_test, np.round(y_pred), average=\"micro\"))\n",
    "    results.append(f1_score(y_test, np.round(y_pred), average=\"micro\"))\n",
    "    results.append(cohen_kappa_score(y_test, np.round(y_pred)))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28056,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "model = make_my_model_multi([100, 100, 100, 100], 6, 18, activation_='relu' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "88/88 [==============================] - 1s 2ms/step - loss: 2.4724 - accuracy: 0.1922\n",
      "Epoch 2/200\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.6377 - accuracy: 0.3953\n",
      "Epoch 3/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 1.3997 - accuracy: 0.4691\n",
      "Epoch 4/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 1.2755 - accuracy: 0.5050\n",
      "Epoch 5/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 1.2194 - accuracy: 0.5136\n",
      "Epoch 6/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 1.1709 - accuracy: 0.5332\n",
      "Epoch 7/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 1.1219 - accuracy: 0.5534\n",
      "Epoch 8/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 1.0856 - accuracy: 0.5659\n",
      "Epoch 9/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 1.0505 - accuracy: 0.5760\n",
      "Epoch 10/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 1.0148 - accuracy: 0.5871\n",
      "Epoch 11/200\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.9837 - accuracy: 0.5956\n",
      "Epoch 12/200\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.9390 - accuracy: 0.6168\n",
      "Epoch 13/200\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.9324 - accuracy: 0.6189\n",
      "Epoch 14/200\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.8928 - accuracy: 0.6324\n",
      "Epoch 15/200\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.8706 - accuracy: 0.6446\n",
      "Epoch 16/200\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.8515 - accuracy: 0.6499\n",
      "Epoch 17/200\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.8254 - accuracy: 0.6600\n",
      "Epoch 18/200\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.8048 - accuracy: 0.6701\n",
      "Epoch 19/200\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.7738 - accuracy: 0.6884\n",
      "Epoch 20/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.7648 - accuracy: 0.6875\n",
      "Epoch 21/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.7478 - accuracy: 0.6928\n",
      "Epoch 22/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.7260 - accuracy: 0.7000\n",
      "Epoch 23/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.7222 - accuracy: 0.7040\n",
      "Epoch 24/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.7093 - accuracy: 0.7052\n",
      "Epoch 25/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.7034 - accuracy: 0.7153\n",
      "Epoch 26/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.6763 - accuracy: 0.7250\n",
      "Epoch 27/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.6858 - accuracy: 0.7177\n",
      "Epoch 28/200\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.7055 - accuracy: 0.7111\n",
      "Epoch 29/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.6586 - accuracy: 0.7370\n",
      "Epoch 30/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.6371 - accuracy: 0.7437\n",
      "Epoch 31/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.6397 - accuracy: 0.7347\n",
      "Epoch 32/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.6292 - accuracy: 0.7384\n",
      "Epoch 33/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.6158 - accuracy: 0.7479\n",
      "Epoch 34/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.6094 - accuracy: 0.7519\n",
      "Epoch 35/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.5959 - accuracy: 0.7552\n",
      "Epoch 36/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.5985 - accuracy: 0.7530\n",
      "Epoch 37/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.5862 - accuracy: 0.7647\n",
      "Epoch 38/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.5867 - accuracy: 0.7574\n",
      "Epoch 39/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.5730 - accuracy: 0.7632\n",
      "Epoch 40/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.5605 - accuracy: 0.7762\n",
      "Epoch 41/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.5535 - accuracy: 0.7745\n",
      "Epoch 42/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.5520 - accuracy: 0.7743\n",
      "Epoch 43/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.5637 - accuracy: 0.7628\n",
      "Epoch 44/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.5590 - accuracy: 0.7700\n",
      "Epoch 45/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.5327 - accuracy: 0.7840\n",
      "Epoch 46/200\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.5270 - accuracy: 0.7868\n",
      "Epoch 47/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.5262 - accuracy: 0.7871\n",
      "Epoch 48/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.5183 - accuracy: 0.7883\n",
      "Epoch 49/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.5082 - accuracy: 0.7940\n",
      "Epoch 50/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.5094 - accuracy: 0.7928\n",
      "Epoch 51/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.5013 - accuracy: 0.7946\n",
      "Epoch 52/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.4929 - accuracy: 0.8002\n",
      "Epoch 53/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.4878 - accuracy: 0.8012\n",
      "Epoch 54/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.4889 - accuracy: 0.8011\n",
      "Epoch 55/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.4936 - accuracy: 0.7975\n",
      "Epoch 56/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.4798 - accuracy: 0.8070\n",
      "Epoch 57/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.4760 - accuracy: 0.8073\n",
      "Epoch 58/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.4682 - accuracy: 0.8110\n",
      "Epoch 59/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.4674 - accuracy: 0.8115\n",
      "Epoch 60/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.4607 - accuracy: 0.8141\n",
      "Epoch 61/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.4670 - accuracy: 0.8120\n",
      "Epoch 62/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.4388 - accuracy: 0.8215\n",
      "Epoch 63/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.4529 - accuracy: 0.8146\n",
      "Epoch 64/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.4400 - accuracy: 0.8247\n",
      "Epoch 65/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.4434 - accuracy: 0.8218\n",
      "Epoch 66/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.4411 - accuracy: 0.8247\n",
      "Epoch 67/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.4309 - accuracy: 0.8285\n",
      "Epoch 68/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.4415 - accuracy: 0.8223\n",
      "Epoch 69/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.4236 - accuracy: 0.8313\n",
      "Epoch 70/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.4383 - accuracy: 0.8184\n",
      "Epoch 71/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.4157 - accuracy: 0.8330\n",
      "Epoch 72/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.4240 - accuracy: 0.8294\n",
      "Epoch 73/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.4145 - accuracy: 0.8327\n",
      "Epoch 74/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.4108 - accuracy: 0.8351\n",
      "Epoch 75/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.4017 - accuracy: 0.8339\n",
      "Epoch 76/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.4047 - accuracy: 0.8402\n",
      "Epoch 77/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.4057 - accuracy: 0.8362\n",
      "Epoch 78/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3990 - accuracy: 0.8428\n",
      "Epoch 79/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.4131 - accuracy: 0.8321\n",
      "Epoch 80/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3897 - accuracy: 0.8440\n",
      "Epoch 81/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3854 - accuracy: 0.8446\n",
      "Epoch 82/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.4047 - accuracy: 0.8373\n",
      "Epoch 83/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3814 - accuracy: 0.8469\n",
      "Epoch 84/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3885 - accuracy: 0.8436\n",
      "Epoch 85/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3804 - accuracy: 0.8492\n",
      "Epoch 86/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3637 - accuracy: 0.8541\n",
      "Epoch 87/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3730 - accuracy: 0.8519\n",
      "Epoch 88/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3853 - accuracy: 0.8451\n",
      "Epoch 89/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3724 - accuracy: 0.8537\n",
      "Epoch 90/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3671 - accuracy: 0.8566\n",
      "Epoch 91/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3634 - accuracy: 0.8556\n",
      "Epoch 92/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3603 - accuracy: 0.8572\n",
      "Epoch 93/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3471 - accuracy: 0.8604\n",
      "Epoch 94/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3513 - accuracy: 0.8619\n",
      "Epoch 95/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3513 - accuracy: 0.8639\n",
      "Epoch 96/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3464 - accuracy: 0.8639\n",
      "Epoch 97/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3466 - accuracy: 0.8660\n",
      "Epoch 98/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3447 - accuracy: 0.8666\n",
      "Epoch 99/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3384 - accuracy: 0.8657\n",
      "Epoch 100/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3433 - accuracy: 0.8604\n",
      "Epoch 101/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3605 - accuracy: 0.8551\n",
      "Epoch 102/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3465 - accuracy: 0.8650\n",
      "Epoch 103/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3303 - accuracy: 0.8657\n",
      "Epoch 104/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3281 - accuracy: 0.8687\n",
      "Epoch 105/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3301 - accuracy: 0.8696\n",
      "Epoch 106/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3307 - accuracy: 0.8723\n",
      "Epoch 107/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3255 - accuracy: 0.8701\n",
      "Epoch 108/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3166 - accuracy: 0.8768\n",
      "Epoch 109/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3208 - accuracy: 0.8724\n",
      "Epoch 110/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3179 - accuracy: 0.8725\n",
      "Epoch 111/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3235 - accuracy: 0.8721\n",
      "Epoch 112/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3104 - accuracy: 0.8777\n",
      "Epoch 113/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3000 - accuracy: 0.8819\n",
      "Epoch 114/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3030 - accuracy: 0.8805\n",
      "Epoch 115/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3055 - accuracy: 0.8833\n",
      "Epoch 116/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2992 - accuracy: 0.8827\n",
      "Epoch 117/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2985 - accuracy: 0.8852\n",
      "Epoch 118/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3018 - accuracy: 0.8819\n",
      "Epoch 119/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2988 - accuracy: 0.8829\n",
      "Epoch 120/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2858 - accuracy: 0.8905\n",
      "Epoch 121/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3034 - accuracy: 0.8824\n",
      "Epoch 122/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2912 - accuracy: 0.8861\n",
      "Epoch 123/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3063 - accuracy: 0.8773\n",
      "Epoch 124/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2984 - accuracy: 0.8798\n",
      "Epoch 125/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2876 - accuracy: 0.8873\n",
      "Epoch 126/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2888 - accuracy: 0.8854\n",
      "Epoch 127/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2780 - accuracy: 0.8950\n",
      "Epoch 128/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2785 - accuracy: 0.8942\n",
      "Epoch 129/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2696 - accuracy: 0.8965\n",
      "Epoch 130/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2983 - accuracy: 0.8804\n",
      "Epoch 131/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2902 - accuracy: 0.8855\n",
      "Epoch 132/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2783 - accuracy: 0.8884\n",
      "Epoch 133/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2778 - accuracy: 0.8914\n",
      "Epoch 134/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2714 - accuracy: 0.8916\n",
      "Epoch 135/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2652 - accuracy: 0.8967\n",
      "Epoch 136/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2710 - accuracy: 0.8927\n",
      "Epoch 137/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2761 - accuracy: 0.8933\n",
      "Epoch 138/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2737 - accuracy: 0.8964\n",
      "Epoch 139/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2642 - accuracy: 0.8967\n",
      "Epoch 140/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2772 - accuracy: 0.8891\n",
      "Epoch 141/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2571 - accuracy: 0.9021\n",
      "Epoch 142/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2508 - accuracy: 0.9013\n",
      "Epoch 143/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2671 - accuracy: 0.8974\n",
      "Epoch 144/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2530 - accuracy: 0.9042\n",
      "Epoch 145/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2457 - accuracy: 0.9054\n",
      "Epoch 146/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2519 - accuracy: 0.9042\n",
      "Epoch 147/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2697 - accuracy: 0.8948\n",
      "Epoch 148/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2569 - accuracy: 0.8982\n",
      "Epoch 149/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2584 - accuracy: 0.8972\n",
      "Epoch 150/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2401 - accuracy: 0.9077\n",
      "Epoch 151/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2419 - accuracy: 0.9052\n",
      "Epoch 152/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2435 - accuracy: 0.9056\n",
      "Epoch 153/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2382 - accuracy: 0.9114\n",
      "Epoch 154/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2413 - accuracy: 0.9072\n",
      "Epoch 155/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2348 - accuracy: 0.9072\n",
      "Epoch 156/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2416 - accuracy: 0.9042\n",
      "Epoch 157/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2519 - accuracy: 0.9021\n",
      "Epoch 158/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2338 - accuracy: 0.9101\n",
      "Epoch 159/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2383 - accuracy: 0.9060\n",
      "Epoch 160/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2535 - accuracy: 0.9013\n",
      "Epoch 161/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2174 - accuracy: 0.9166: 0s - loss: 0.2097 - accuracy: 0.\n",
      "Epoch 162/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2251 - accuracy: 0.9129\n",
      "Epoch 163/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2368 - accuracy: 0.9081\n",
      "Epoch 164/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2446 - accuracy: 0.9022\n",
      "Epoch 165/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2244 - accuracy: 0.9136\n",
      "Epoch 166/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2499 - accuracy: 0.8985\n",
      "Epoch 167/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2398 - accuracy: 0.9066\n",
      "Epoch 168/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2187 - accuracy: 0.9155\n",
      "Epoch 169/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2439 - accuracy: 0.9033\n",
      "Epoch 170/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2101 - accuracy: 0.9204\n",
      "Epoch 171/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2222 - accuracy: 0.9150\n",
      "Epoch 172/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2155 - accuracy: 0.9181\n",
      "Epoch 173/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2095 - accuracy: 0.9200\n",
      "Epoch 174/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2228 - accuracy: 0.9141\n",
      "Epoch 175/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2187 - accuracy: 0.9136\n",
      "Epoch 176/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2163 - accuracy: 0.9159\n",
      "Epoch 177/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2024 - accuracy: 0.9226\n",
      "Epoch 178/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1956 - accuracy: 0.9265\n",
      "Epoch 179/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2115 - accuracy: 0.9148\n",
      "Epoch 180/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2059 - accuracy: 0.9176\n",
      "Epoch 181/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1986 - accuracy: 0.9244\n",
      "Epoch 182/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2045 - accuracy: 0.9195\n",
      "Epoch 183/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2043 - accuracy: 0.9217\n",
      "Epoch 184/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2019 - accuracy: 0.9220\n",
      "Epoch 185/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2120 - accuracy: 0.9186\n",
      "Epoch 186/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2066 - accuracy: 0.9205\n",
      "Epoch 187/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2145 - accuracy: 0.9167\n",
      "Epoch 188/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2089 - accuracy: 0.9209\n",
      "Epoch 189/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1935 - accuracy: 0.9276\n",
      "Epoch 190/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1955 - accuracy: 0.9246\n",
      "Epoch 191/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2002 - accuracy: 0.9246\n",
      "Epoch 192/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1973 - accuracy: 0.9220\n",
      "Epoch 193/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1928 - accuracy: 0.9282\n",
      "Epoch 194/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2010 - accuracy: 0.9226\n",
      "Epoch 195/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1877 - accuracy: 0.9255\n",
      "Epoch 196/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1874 - accuracy: 0.9272\n",
      "Epoch 197/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2009 - accuracy: 0.9240\n",
      "Epoch 198/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1922 - accuracy: 0.9246\n",
      "Epoch 199/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1905 - accuracy: 0.9265\n",
      "Epoch 200/200\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2101 - accuracy: 0.9174\n"
     ]
    }
   ],
   "source": [
    "y_pred = compile_fit_multiclass(model, X_train, X_test, y_train, 256, 200, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[580   0   2   1   0   0   0   1   0   1   1   0   0   0   0   0   0   0]\n",
      " [  1 220   0   0   1   0   0  35   0  35   2   0   4   0   0   0   0   0]\n",
      " [  0   0 468   0   0   0   0  11   0   0   0   0  62   5   0  41   0   0]\n",
      " [  0   0   0 378   0   0  30   0   0   0   0   9   0   0   0   0   0   0]\n",
      " [  0   0   0   0  62   5   0   0   0   0   7   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   1  36   0   0   0   1   0   0   0   0   4   0   0   0]\n",
      " [  0   0   0  55   0   0 809   0   0   0   0   0   0  82   0   6   0   0]\n",
      " [  0  30   5   0   0   0   0 255   0   8   0   0  35   0   0   0   0   1]\n",
      " [  1   0   0   0   0   0   0   0  11   1   0   0   0   0   0   0   0   2]\n",
      " [  0  17   0   0   1   0   0   3   0  92   8   0   1   0   0   0   1   0]\n",
      " [  0   2   0   0  15   2   0   3   0  23 103   0   0   0   0   0   0   0]\n",
      " [  0   0   0   5   0   0   0   0   0   0   0  60   0   0   0   0   0   0]\n",
      " [  1   2  29   0   0   0   0  20   0   3   0   0 273   0   0   7   0   0]\n",
      " [  1   0   6   0   0   0  65   0   0   0   0   0   2 708   0  83   0   0]\n",
      " [  1   0   0   0   0   0   0   0   0   0   0   0   0   0   9   0   0   0]\n",
      " [  2   0  61   0   0   0   5   3   0   0   0   0   7  43   0 578   0   0]\n",
      " [  2   0   0   0   0   0   0   0   0   0   0   0   0   0   2   0  53   0]\n",
      " [  1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   4]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, \\\n",
    "f1_score, cohen_kappa_score, recall_score\n",
    "\n",
    "confusion = confusion_matrix(np.argmax(y_pred, axis = 1), np.argmax(y_test, axis = 1))\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8373129009265858,\n",
       " 0.8373129009265858,\n",
       " 0.8373129009265857,\n",
       " 0.8180928161347001]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics_multiclass(np.argmax(y_pred, axis = 1), np.argmax(y_test, axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class=0, n=2796 (9.966%)\n",
      "Class=17, n=27 (0.096%)\n",
      "Class=8, n=78 (0.278%)\n",
      "Class=16, n=246 (0.877%)\n",
      "Class=14, n=81 (0.289%)\n",
      "Class=5, n=198 (0.706%)\n",
      "Class=4, n=471 (1.679%)\n",
      "Class=10, n=592 (2.110%)\n",
      "Class=9, n=683 (2.434%)\n",
      "Class=1, n=1433 (5.108%)\n",
      "Class=7, n=1712 (6.102%)\n",
      "Class=12, n=1985 (7.075%)\n",
      "Class=2, n=2854 (10.173%)\n",
      "Class=15, n=3597 (12.821%)\n",
      "Class=13, n=4194 (14.949%)\n",
      "Class=6, n=4553 (16.228%)\n",
      "Class=3, n=2166 (7.720%)\n",
      "Class=11, n=390 (1.390%)\n"
     ]
    }
   ],
   "source": [
    "counter = Counter(y)\n",
    "for k,v in counter.items():\n",
    "\tper = v / len(y) * 100\n",
    "\tprint('Class=%d, n=%d (%.3f%%)' % (k, v, per))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((22444, 6), (22444,))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, np.argmax(y_train, axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((65592, 6), (65592, 18))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_smote.shape, y_train_smote.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 100)               700       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 18)                1818      \n",
      "=================================================================\n",
      "Total params: 32,818\n",
      "Trainable params: 32,818\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "model = make_my_model_multi([100, 100, 100, 100], 6, 18, activation_='relu' )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((22444, 6), (65563, 6), (22444, 18), (65563, 18), (5612, 6), (5612, 18))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_train_smote.shape, y_train.shape, y_train_smote.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function compile_fit_multiclass in module __main__:\n",
      "\n",
      "compile_fit_multiclass(model, X_train, X_test, y_train, batch, epochs, verbose=0)\n",
      "    #En este caso, usamos categorical_crossentropy como función de coste y \n",
      "    #permitimos elegir otro batch para poder hacer uso de\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(compile_fit_multiclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "231/231 [==============================] - 1s 2ms/step - loss: 0.1681 - accuracy: 0.9377 - val_loss: 0.2655 - val_accuracy: 0.9166\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26111\n",
      "Epoch 2/100\n",
      "231/231 [==============================] - 0s 2ms/step - loss: 0.1465 - accuracy: 0.9483 - val_loss: 0.2820 - val_accuracy: 0.9059\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.26111\n",
      "Epoch 3/100\n",
      "231/231 [==============================] - 0s 2ms/step - loss: 0.1529 - accuracy: 0.9446 - val_loss: 0.2888 - val_accuracy: 0.9016\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.26111\n",
      "Epoch 4/100\n",
      "231/231 [==============================] - 0s 2ms/step - loss: 0.1591 - accuracy: 0.9417 - val_loss: 0.2795 - val_accuracy: 0.9123\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.26111\n",
      "Epoch 5/100\n",
      "231/231 [==============================] - 0s 2ms/step - loss: 0.1533 - accuracy: 0.9452 - val_loss: 0.2849 - val_accuracy: 0.9097\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.26111\n",
      "Epoch 6/100\n",
      "231/231 [==============================] - 0s 2ms/step - loss: 0.1549 - accuracy: 0.9442 - val_loss: 0.2853 - val_accuracy: 0.9125\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.26111\n",
      "Epoch 7/100\n",
      "231/231 [==============================] - 0s 2ms/step - loss: 0.1518 - accuracy: 0.9448 - val_loss: 0.2737 - val_accuracy: 0.9123\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.26111\n",
      "Epoch 8/100\n",
      "231/231 [==============================] - 0s 2ms/step - loss: 0.1503 - accuracy: 0.9454 - val_loss: 0.2806 - val_accuracy: 0.9079\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.26111\n",
      "Epoch 9/100\n",
      "231/231 [==============================] - 0s 2ms/step - loss: 0.1528 - accuracy: 0.9438 - val_loss: 0.2700 - val_accuracy: 0.9129\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.26111\n",
      "Epoch 10/100\n",
      "231/231 [==============================] - 0s 2ms/step - loss: 0.1552 - accuracy: 0.9437 - val_loss: 0.2996 - val_accuracy: 0.9030\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.26111\n",
      "Epoch 11/100\n",
      "231/231 [==============================] - 0s 2ms/step - loss: 0.1521 - accuracy: 0.9438 - val_loss: 0.2682 - val_accuracy: 0.9152\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.26111\n",
      "Epoch 00011: early stopping\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(\n",
    "    seed\n",
    ")\n",
    "y_pred_smote = compile_fit_multiclass(model, X_train_smote, X_test, y_train_smote, 256, 100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "513/513 [==============================] - 1s 1ms/step - loss: 0.7905 - accuracy: 0.7641\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7efdb6e90290>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_test_smote, y_test_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5612, 18), (5612, 18))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_smote.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[569   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  1 244   0   0   0   0   0   9   0   5   2   0   1   0   0   0   0   0]\n",
      " [  0   0 529   0   0   0   1   2   0   0   0   0  26   2   0  67   0   0]\n",
      " [  0   0   0 424   0   0  55   0   0   0   0   1   0   0   0   0   0   0]\n",
      " [  3   0   0   0  77   1   0   0   0   0   1   0   0   0   0   0   0   0]\n",
      " [  2   0   0   0   0  42   0   0   0   0   1   0   0   0   1   0   0   0]\n",
      " [  0   0   0  12   0   0 806   0   0   0   0   0   0  57   0   1   0   0]\n",
      " [  0  16   3   0   2   0   0 307   0   0   2   0  12   0   0   1   0   0]\n",
      " [  0   0   0   0   0   0   0   0  11   0   0   0   0   0   0   0   0   0]\n",
      " [  7  10   1   0   0   0   0   1   0 155   3   0   1   0   0   0   0   0]\n",
      " [  1   0   0   0   1   0   0   1   0   4 112   0   0   0   0   0   0   0]\n",
      " [  0   0   0   3   0   0   0   0   0   0   0  68   0   0   0   0   0   0]\n",
      " [  0   1  14   0   0   0   0  11   0   0   0   0 343   1   0   4   0   0]\n",
      " [  0   0   0   0   0   0  45   0   0   0   0   0   0 735   0  50   0   0]\n",
      " [  1   0   0   0   0   0   0   0   0   0   0   0   0   0  14   0   0   0]\n",
      " [  0   0  24   0   0   0   2   0   0   0   0   0   1  43   0 592   0   0]\n",
      " [  4   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  54   0]\n",
      " [  2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   7]]\n"
     ]
    }
   ],
   "source": [
    "confusion = confusion_matrix(np.argmax(y_pred_smote, axis = 1), np.argmax(y_test, axis = 1))\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9068068424803991,\n",
       " 0.9068068424803991,\n",
       " 0.9068068424803991,\n",
       " 0.8960528113126178]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics_multiclass(np.argmax(y_pred_smote, axis = 1), np.argmax(y_test, axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 6, 2, 2])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y_pred_smote, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save(\"de_momento_mejor.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('tf_gpu': conda)",
   "language": "python",
   "name": "python3710jvsc74a57bd03498809ad0933a938773cf5296d2f109940af130e065f95298c4036d7f924fb9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "3498809ad0933a938773cf5296d2f109940af130e065f95298c4036d7f924fb9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
