{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97985785",
   "metadata": {},
   "source": [
    "# Gui√≥n 2\n",
    "Antonio Ruiz Molero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2efb750c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "white = pd.read_csv(\"winequality-white.csv\", sep = \";\")\n",
    "\n",
    "red = pd.read_csv(\"winequality-red.csv\", sep = \";\")\n",
    "\n",
    "red['type'] = 1\n",
    "\n",
    "white['type'] = 0\n",
    "\n",
    "wines = red.append(white, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9d02a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.215307</td>\n",
       "      <td>0.339666</td>\n",
       "      <td>0.318633</td>\n",
       "      <td>5.443235</td>\n",
       "      <td>0.056034</td>\n",
       "      <td>30.525319</td>\n",
       "      <td>115.744574</td>\n",
       "      <td>0.994697</td>\n",
       "      <td>3.218501</td>\n",
       "      <td>0.531268</td>\n",
       "      <td>10.491801</td>\n",
       "      <td>5.818378</td>\n",
       "      <td>0.246114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.296434</td>\n",
       "      <td>0.164636</td>\n",
       "      <td>0.145318</td>\n",
       "      <td>4.757804</td>\n",
       "      <td>0.035034</td>\n",
       "      <td>17.749400</td>\n",
       "      <td>56.521855</td>\n",
       "      <td>0.002999</td>\n",
       "      <td>0.160787</td>\n",
       "      <td>0.148806</td>\n",
       "      <td>1.192712</td>\n",
       "      <td>0.873255</td>\n",
       "      <td>0.430779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.800000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.987110</td>\n",
       "      <td>2.720000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.038000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>0.992340</td>\n",
       "      <td>3.110000</td>\n",
       "      <td>0.430000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.047000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>0.994890</td>\n",
       "      <td>3.210000</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>10.300000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.700000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>8.100000</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>0.996990</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>11.300000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15.900000</td>\n",
       "      <td>1.580000</td>\n",
       "      <td>1.660000</td>\n",
       "      <td>65.800000</td>\n",
       "      <td>0.611000</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>440.000000</td>\n",
       "      <td>1.038980</td>\n",
       "      <td>4.010000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>14.900000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "count    6497.000000       6497.000000  6497.000000     6497.000000   \n",
       "mean        7.215307          0.339666     0.318633        5.443235   \n",
       "std         1.296434          0.164636     0.145318        4.757804   \n",
       "min         3.800000          0.080000     0.000000        0.600000   \n",
       "25%         6.400000          0.230000     0.250000        1.800000   \n",
       "50%         7.000000          0.290000     0.310000        3.000000   \n",
       "75%         7.700000          0.400000     0.390000        8.100000   \n",
       "max        15.900000          1.580000     1.660000       65.800000   \n",
       "\n",
       "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
       "count  6497.000000          6497.000000           6497.000000  6497.000000   \n",
       "mean      0.056034            30.525319            115.744574     0.994697   \n",
       "std       0.035034            17.749400             56.521855     0.002999   \n",
       "min       0.009000             1.000000              6.000000     0.987110   \n",
       "25%       0.038000            17.000000             77.000000     0.992340   \n",
       "50%       0.047000            29.000000            118.000000     0.994890   \n",
       "75%       0.065000            41.000000            156.000000     0.996990   \n",
       "max       0.611000           289.000000            440.000000     1.038980   \n",
       "\n",
       "                pH    sulphates      alcohol      quality         type  \n",
       "count  6497.000000  6497.000000  6497.000000  6497.000000  6497.000000  \n",
       "mean      3.218501     0.531268    10.491801     5.818378     0.246114  \n",
       "std       0.160787     0.148806     1.192712     0.873255     0.430779  \n",
       "min       2.720000     0.220000     8.000000     3.000000     0.000000  \n",
       "25%       3.110000     0.430000     9.500000     5.000000     0.000000  \n",
       "50%       3.210000     0.510000    10.300000     6.000000     0.000000  \n",
       "75%       3.320000     0.600000    11.300000     6.000000     0.000000  \n",
       "max       4.010000     2.000000    14.900000     9.000000     1.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wines.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb3c0826",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = wines.iloc[:, 0:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e47be57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Y = np.ravel(wines.quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c04f7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5405d295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.14247327,  2.18883292, -2.19283252, ...,  0.19309677,\n",
       "        -0.91546416, -0.93722961],\n",
       "       [ 0.45103572,  3.28223494, -2.19283252, ...,  0.99957862,\n",
       "        -0.58006813, -0.93722961],\n",
       "       [ 0.45103572,  2.55330026, -1.91755268, ...,  0.79795816,\n",
       "        -0.58006813, -0.93722961],\n",
       "       ...,\n",
       "       [-0.55179227, -0.6054167 , -0.88525328, ..., -0.47897144,\n",
       "        -0.91546416,  0.20799905],\n",
       "       [-1.32319841, -0.30169391, -0.12823371, ..., -1.016626  ,\n",
       "         1.9354021 ,  1.35322771],\n",
       "       [-0.93749534, -0.78765037,  0.42232597, ..., -1.41986693,\n",
       "         1.09691202,  0.20799905]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63df50b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6d91f8",
   "metadata": {},
   "source": [
    "Definimos una funci√≥n que nos ayude a automatizar el proceso de creaci√≥n de modelos, al igual que en la pr√°ctica anterior. Como par√°metros de entrada, podemos definir el numero de capas ocultas y las neuronas por capa. Adem√°s del n√∫mero de particiones, √©pocas, el optimizador y la m√©trica que usa durante el aprendizaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4715eb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_model(units_per_layer, input_s,  n_splits_, epochs_, verbose_=1, seed_=1, optimizer_ = 'rmsprop', metrics_ = [ 'mae'], activation_ = 'relu' ):\n",
    "    seed = seed_\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=n_splits_, shuffle=True, random_state=seed)\n",
    "\n",
    "    for train, test in kfold.split(X, Y):\n",
    "        model = Sequential()\n",
    "        depth = len(units_per_layer)\n",
    "        model.add(Dense(units_per_layer[0], activation=activation_, input_shape=(input_s,)))\n",
    "        if (depth!=0):\n",
    "            for i in range(1, depth):\n",
    "                model.add(Dense(units_per_layer[i], activation=activation_))\n",
    "        model.add(Dense(1))\n",
    "        \n",
    "        model.compile(optimizer=optimizer_, loss='mse', metrics=metrics_)\n",
    "        model.fit(X_train[train], Y[train], epochs=epochs_, verbose=verbose_)\n",
    "        y_pred=model.predict(X_train[test])\n",
    "\n",
    "    mse_value, mae_value = model.evaluate(X_train[test], Y[test], verbose=verbose_)\n",
    "    r2score = r2_score(Y[test], y_pred)\n",
    "    return mse_value, mae_value, r2score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8899b392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.008574560284614563, 0.0696149617433548, 0.9887701004304583)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_model([62], 12, 5, 10, verbose_=0, seed_=1, optimizer_='rmsprop', metrics_=['mae'], activation_='relu' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbe62a1",
   "metadata": {},
   "source": [
    "## Resultados iniciales\n",
    "Usando la configuraci√≥n de 1 capa oculta con 62 neuronas, vemos la variaci√≥n de los resultados al usar distintas √©pocas con las m√©tricas mse, mae y R2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c23a91be",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [10, 20, 30, 40, 50]\n",
    "metricas = []\n",
    "for i_epochs in epochs:\n",
    "    metricas.append(regression_model([62], 12, 5, i_epochs, verbose_=0, seed_=1, optimizer_='rmsprop', metrics_=['mae'], activation_='relu' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "525b32f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.013785162940621376, 0.09119463711977005, 0.9819458944296605),\n",
       " (0.0031737484969198704, 0.042470671236515045, 0.9958434127049484),\n",
       " (0.0023599134292453527, 0.03551211208105087, 0.996909276122736),\n",
       " (0.0021134091075509787, 0.03185389190912247, 0.9972321161603671),\n",
       " (0.00415986031293869, 0.05721772089600563, 0.994551927900059)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d67b164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                mse       mae        R2\n",
      "10 epochs  0.013785  0.091195  0.981946\n",
      "20 epochs  0.003174  0.042471  0.995843\n",
      "30 epochs  0.002360  0.035512  0.996909\n",
      "40 epochs  0.002113  0.031854  0.997232\n",
      "50 epochs  0.004160  0.057218  0.994552\n"
     ]
    }
   ],
   "source": [
    "df_metricas = pd.DataFrame(metricas, columns=['mse', 'mae', 'R2'], index=['10 epochs', '20 epochs', '30 epochs', '40 epochs', '50 epochs'])\n",
    "print(df_metricas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9a87c5",
   "metadata": {},
   "source": [
    "El n√∫mero de iteraciones que mejores resultados obtienen son 40 √©pocas. El valor de mse y mae decae hasta un m√≠nimo en las pruebas que hemos realizado y R2 obtiene un valor muy cercano a 1. Parece que obtenemos un m√≠nimo ya que a valores menores y mayores de √©pocas, los resultados empeoran."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40036f78",
   "metadata": {},
   "source": [
    "## Ajuste fino de la regresi√≥n\n",
    "\n",
    "### Agregar m√°s capas\n",
    "Definimos varios modelos en los cuales aumentamos el n√∫mero de capas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "49cae8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "metricas = []\n",
    "modelos = [[62], [62, 62], [62, 62, 62], [62, 62, 62, 62], [62, 62, 62, 62, 62]]\n",
    "for i_modelos in modelos:\n",
    "    metricas.append(regression_model(i_modelos, 12, 5, 40, verbose_=0, seed_=1, optimizer_='rmsprop', metrics_=['mae'], activation_='relu' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "503f81b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0011985173914581537, 0.026208890601992607, 0.9984303253984265),\n",
       " (0.002360659884288907, 0.03976006060838699, 0.9969082992724735),\n",
       " (0.011676796711981297, 0.09639350324869156, 0.984707167042099),\n",
       " (0.0015980659518390894, 0.033231861889362335, 0.9979070536381559),\n",
       " (0.008068920113146305, 0.08572115749120712, 0.989432321445366)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "53e6d78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      mse       mae        R2\n",
      "1 capa oculta    0.001199  0.026209  0.998430\n",
      "2 capas ocultas  0.002361  0.039760  0.996908\n",
      "3 capas ocultas  0.011677  0.096394  0.984707\n",
      "4 capas ocultas  0.001598  0.033232  0.997907\n",
      "5 capas ocultas  0.008069  0.085721  0.989432\n"
     ]
    }
   ],
   "source": [
    "df_metricas = pd.DataFrame(metricas, columns=['mse', 'mae', 'R2'], index=['1 capa oculta', '2 capas ocultas', '3 capas ocultas', '4 capas ocultas', '5 capas ocultas'])\n",
    "print(df_metricas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1420a83",
   "metadata": {},
   "source": [
    "Los resultados son curiosos. En general, parece que a mayor n√∫mero de capas ocultas, el resultado empeora, ya que con una capa oculta obtenemos muy buenos valores en las m√©tricas utilizadas. Pero destaca el caso con 4 capas ocultas en las que se obtienen resultados muy parecidos a una solo capa. Ya que los resultados son ligeramente peores, elegir√≠a el modelo de una capa oculta para este problema ya que da los mejores resultados con muy pocos par√°metros y, por consiguiente, menor tiempo de c√≥mputo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5622cc",
   "metadata": {},
   "source": [
    "### Agregar m√°s neuronas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6f504221",
   "metadata": {},
   "outputs": [],
   "source": [
    "metricas = []\n",
    "neuronas = [32, 64, 128]\n",
    "for n_neurons in neuronas:\n",
    "    modelos = [[n_neurons], [n_neurons, n_neurons], [n_neurons, n_neurons, n_neurons], [n_neurons, n_neurons, n_neurons, n_neurons]]\n",
    "    for i_modelos in modelos:\n",
    "            metricas.append(regression_model(i_modelos, 12, 5, 40, verbose_=0, seed_=1, optimizer_='rmsprop', metrics_=['mae'], activation_='relu' ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b96461c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "metricas_mas_neuronas = metricas.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ab3b6986",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_neurons = []\n",
    "col_capas = []\n",
    "\n",
    "neuronas = [32, 64, 128]\n",
    "for n_neurons in neuronas:\n",
    "    modelos = [[n_neurons], [n_neurons, n_neurons], [n_neurons, n_neurons, n_neurons], [n_neurons, n_neurons, n_neurons, n_neurons]]\n",
    "    for i_modelos in modelos:\n",
    "            col_neurons.append(n_neurons)\n",
    "            col_capas.append(len(i_modelos))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9363751b",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_neurons = np.array(col_neurons)\n",
    "col_capas = np.array(col_capas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e9314295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0030147526413202286, 0.04196714609861374, 0.9960516490405238),\n",
       " (0.0021234694868326187, 0.03902815654873848, 0.9972189402503031),\n",
       " (0.00232324399985373, 0.034287091344594955, 0.9969572995729771),\n",
       " (0.006025801412761211, 0.06982725113630295, 0.9921081461392257),\n",
       " (0.0017118020914494991, 0.03035326488316059, 0.997758092512183),\n",
       " (0.0025139683857560158, 0.043074388056993484, 0.9967075139799236),\n",
       " (0.0026746534276753664, 0.04470125213265419, 0.9964970685803987),\n",
       " (0.005054554436355829, 0.06605581194162369, 0.9933801723098398),\n",
       " (0.0017037560464814305, 0.028759095817804337, 0.9977686257101368),\n",
       " (0.02868947759270668, 0.1408333033323288, 0.9624260407298256),\n",
       " (0.004920251201838255, 0.05909089371562004, 0.9935560614759515),\n",
       " (0.0009687933488748968, 0.024702884256839752, 0.9987311844307625)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metricas_mas_neuronas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "254ec151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         mse       mae        R2\n",
      "0   0.003015  0.041967  0.996052\n",
      "1   0.002123  0.039028  0.997219\n",
      "2   0.002323  0.034287  0.996957\n",
      "3   0.006026  0.069827  0.992108\n",
      "4   0.001712  0.030353  0.997758\n",
      "5   0.002514  0.043074  0.996708\n",
      "6   0.002675  0.044701  0.996497\n",
      "7   0.005055  0.066056  0.993380\n",
      "8   0.001704  0.028759  0.997769\n",
      "9   0.028689  0.140833  0.962426\n",
      "10  0.004920  0.059091  0.993556\n",
      "11  0.000969  0.024703  0.998731\n"
     ]
    }
   ],
   "source": [
    "df_aux1 = pd.DataFrame(metricas_mas_neuronas, columns=['mse', 'mae', 'R2'])\n",
    "print(df_aux1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "25e353f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Capas  Neuronas       mse       mae        R2\n",
      "0       1        32  0.003015  0.041967  0.996052\n",
      "1       2        32  0.002123  0.039028  0.997219\n",
      "2       3        32  0.002323  0.034287  0.996957\n",
      "3       4        32  0.006026  0.069827  0.992108\n",
      "4       1        64  0.001712  0.030353  0.997758\n",
      "5       2        64  0.002514  0.043074  0.996708\n",
      "6       3        64  0.002675  0.044701  0.996497\n",
      "7       4        64  0.005055  0.066056  0.993380\n",
      "8       1       128  0.001704  0.028759  0.997769\n",
      "9       2       128  0.028689  0.140833  0.962426\n",
      "10      3       128  0.004920  0.059091  0.993556\n",
      "11      4       128  0.000969  0.024703  0.998731\n"
     ]
    }
   ],
   "source": [
    "data = {'Capas': col_capas,\n",
    "        'Neuronas': col_neurons\n",
    "        }\n",
    "df_aux2 = pd.DataFrame(data, columns=['Capas', 'Neuronas'])\n",
    "df_mas_neuronas = pd.concat([df_aux2, df_aux1], axis=1)\n",
    "\n",
    "print(df_mas_neuronas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570f4f72",
   "metadata": {},
   "source": [
    "Podemos ver cual fue el caso que mejor resultado dio si ordenamos de mayor a menor los casos en funci√≥n de las m√©tricas obtenidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "501f726a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Capas</th>\n",
       "      <th>Neuronas</th>\n",
       "      <th>mse</th>\n",
       "      <th>mae</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.024703</td>\n",
       "      <td>0.998731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>0.001704</td>\n",
       "      <td>0.028759</td>\n",
       "      <td>0.997769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>0.001712</td>\n",
       "      <td>0.030353</td>\n",
       "      <td>0.997758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>0.002123</td>\n",
       "      <td>0.039028</td>\n",
       "      <td>0.997219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>0.002323</td>\n",
       "      <td>0.034287</td>\n",
       "      <td>0.996957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>0.002514</td>\n",
       "      <td>0.043074</td>\n",
       "      <td>0.996708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>0.002675</td>\n",
       "      <td>0.044701</td>\n",
       "      <td>0.996497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>0.003015</td>\n",
       "      <td>0.041967</td>\n",
       "      <td>0.996052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>128</td>\n",
       "      <td>0.004920</td>\n",
       "      <td>0.059091</td>\n",
       "      <td>0.993556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>0.005055</td>\n",
       "      <td>0.066056</td>\n",
       "      <td>0.993380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>0.006026</td>\n",
       "      <td>0.069827</td>\n",
       "      <td>0.992108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>0.028689</td>\n",
       "      <td>0.140833</td>\n",
       "      <td>0.962426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Capas  Neuronas       mse       mae        R2\n",
       "11      4       128  0.000969  0.024703  0.998731\n",
       "8       1       128  0.001704  0.028759  0.997769\n",
       "4       1        64  0.001712  0.030353  0.997758\n",
       "1       2        32  0.002123  0.039028  0.997219\n",
       "2       3        32  0.002323  0.034287  0.996957\n",
       "5       2        64  0.002514  0.043074  0.996708\n",
       "6       3        64  0.002675  0.044701  0.996497\n",
       "0       1        32  0.003015  0.041967  0.996052\n",
       "10      3       128  0.004920  0.059091  0.993556\n",
       "7       4        64  0.005055  0.066056  0.993380\n",
       "3       4        32  0.006026  0.069827  0.992108\n",
       "9       2       128  0.028689  0.140833  0.962426"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mas_neuronas.sort_values(['mse', 'mae', 'R2' ],\n",
    "                                     ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3a1218",
   "metadata": {},
   "source": [
    "Vemos que el mejor resultado se da con 4 capas de 128 neuronas, donde obtenemos los menores valores para mse y mae y casi 1 para R2.\n",
    "\n",
    "Hemos de tener cuidado y no suponer que aumentar el tama√±o de la red mediante el uso de m√°s neuronas y capas ofrecer√°n mejores resultados. Un n√∫mero muy grande de hiperpar√°metros, fruto de muchas capas y neuronas, genera un espacio de soluciones muy grande para el cual los datos de entrenamiento sean insuficientes como para explorar los m√≠nimos de la funci√≥n de coste.\n",
    "\n",
    "En otros casos, un n√∫mero muy grande de par√°metros, pero no demasiado grande, puede provocar sobreaprendizaje. El modelo tiene los suficientes grados de libertad de forma que aprenda el conjunto de entrenamiento *perfectamente*, pero pierda la capacidad de generalizaci√≥n y desempe√±e peor en el conjunto test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf38897",
   "metadata": {},
   "source": [
    "### Par√°metros de optimizaci√≥n\n",
    "\n",
    "Usando la mejor configuraci√≥n anterior, es decir, 4 capas con 128 neuronas, probaremos distintos optimizadores. Cambiamos el optimizador RMSprop por sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5749584f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop, SGD\n",
    "lr = [0.1, 0.01, 0.001, 0.0001]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eac4ce2",
   "metadata": {},
   "source": [
    "### RMSprop con varios valores de learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7921707f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metricas = []\n",
    "for lr_i in lr:\n",
    "    rmsprop = RMSprop(lr= lr_i)\n",
    "    metricas.append(regression_model([128, 128, 128, 128], 12, 5, 40, verbose_=0, seed_=1, optimizer_=rmsprop, metrics_=['mae'], activation_='relu' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "04d17167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.7687675952911377, 0.666232705116272, -0.006837203034204764),\n",
       " (0.014504609629511833, 0.11104712635278702, 0.9810036453834475),\n",
       " (0.005339370109140873, 0.06758232414722443, 0.9930071669104246),\n",
       " (0.006953633856028318, 0.06236644834280014, 0.9908929843271151)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metricas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f57e3f",
   "metadata": {},
   "source": [
    "### SGD con varios valores de learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "51c9712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metricas2 = []\n",
    "for lr_i in lr:\n",
    "    sgd = SGD(lr= lr_i)\n",
    "    metricas2.append(regression_model([128, 128, 128, 128], 12, 5, 40, verbose_=0, seed_=1, optimizer_=sgd, metrics_=['mae'], activation_='relu' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "61d8d638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0033215899020433426, 0.0464121475815773, 0.995649789133669),\n",
       " (0.002909409813582897, 0.039800677448511124, 0.9961896106028529),\n",
       " (0.013721665367484093, 0.08684557676315308, 0.9820290465498708),\n",
       " (0.3130703270435333, 0.4393954277038574, 0.589978928040176)]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metricas2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1ee10b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate([lr, lr])\n",
    "methods = np.concatenate( [[\"RMSProp\"]*4, ['SGD']*4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9f94e44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index      lr Optimizer  index       mse       mae        R2\n",
      "0      0  0.1000   RMSProp      0  0.768768  0.666233 -0.006837\n",
      "1      1  0.0100   RMSProp      1  0.014505  0.111047  0.981004\n",
      "2      2  0.0010   RMSProp      2  0.005339  0.067582  0.993007\n",
      "3      3  0.0001   RMSProp      3  0.006954  0.062366  0.990893\n",
      "4      4  0.1000       SGD      0  0.003322  0.046412  0.995650\n",
      "5      5  0.0100       SGD      1  0.002909  0.039801  0.996190\n",
      "6      6  0.0010       SGD      2  0.013722  0.086846  0.982029\n",
      "7      7  0.0001       SGD      3  0.313070  0.439395  0.589979\n"
     ]
    }
   ],
   "source": [
    "df_aux1 = pd.DataFrame(metricas, columns=['mse', 'mae', 'R2'])\n",
    "df_aux2 = pd.DataFrame(metricas2, columns=['mse', 'mae', 'R2'])\n",
    "data = {'lr': np.concatenate([lr,lr]),\n",
    "        'Optimizer': methods\n",
    "        }\n",
    "df_aux3 = pd.DataFrame(data, columns=['lr', 'Optimizer'])\n",
    "df_aux4 = pd.concat([df_aux1, df_aux2], axis=0)\n",
    "\n",
    "df_optimizers = pd.concat([df_aux3.reset_index(), df_aux4.reset_index()], axis=1, sort=False)\n",
    "print(df_optimizers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4503f9dc",
   "metadata": {},
   "source": [
    "Vemos que en el caso del optimizador RMSProp, el valor 0.001 proporciona el mejor resultado, aunque es muy parecido al obtenido con 0.0001. En cambio, para SGD, el valor √≥ptimo es 0.01.\n",
    "\n",
    "### Conclusiones\n",
    "Elegir√≠amos, por tanto, un modelo con 4 capas ocultas de 128 neuronas donde utilizaremos el optimizador rmsprop con los valores por defecto, ya que es el que mejor resultado ha producido. Sin embargo, podr√≠amos usar SGD con un learning rate $\\alpha = 0.01$ ya que produce resultados muy parecidos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd04b116",
   "metadata": {},
   "source": [
    "# Problema con dataset opcional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039225c1",
   "metadata": {},
   "source": [
    "Volvemos a usar el dataset *glass*. Intentaremos, de nuevo, predecir el tipo de vidrio en funci√≥n de las otras variables. En cuanto a preprocesamiento, volveremos a quitar la columna id y reescalaremos con zscore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d2797af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "glass = pd.read_csv('glass.data', sep=',', header=None)\n",
    "\n",
    "\n",
    "glass = glass.set_axis([\"id\", \n",
    "                \"RI\",\n",
    "                \"Na\",\n",
    "                \"Mg\",\n",
    "                \"Al\",\n",
    "                \"Si\",\n",
    "                \"K\",\n",
    "                \"Ca\",\n",
    "                \"Ba\",\n",
    "                \"Fe\",\n",
    "                \"Type\"], axis=1, inplace=False)\n",
    "glass.head()\n",
    "del glass['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "284f66ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RI</th>\n",
       "      <th>Na</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Al</th>\n",
       "      <th>Si</th>\n",
       "      <th>K</th>\n",
       "      <th>Ca</th>\n",
       "      <th>Ba</th>\n",
       "      <th>Fe</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.52101</td>\n",
       "      <td>13.64</td>\n",
       "      <td>4.49</td>\n",
       "      <td>1.10</td>\n",
       "      <td>71.78</td>\n",
       "      <td>0.06</td>\n",
       "      <td>8.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.51761</td>\n",
       "      <td>13.89</td>\n",
       "      <td>3.60</td>\n",
       "      <td>1.36</td>\n",
       "      <td>72.73</td>\n",
       "      <td>0.48</td>\n",
       "      <td>7.83</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.51618</td>\n",
       "      <td>13.53</td>\n",
       "      <td>3.55</td>\n",
       "      <td>1.54</td>\n",
       "      <td>72.99</td>\n",
       "      <td>0.39</td>\n",
       "      <td>7.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.51766</td>\n",
       "      <td>13.21</td>\n",
       "      <td>3.69</td>\n",
       "      <td>1.29</td>\n",
       "      <td>72.61</td>\n",
       "      <td>0.57</td>\n",
       "      <td>8.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.51742</td>\n",
       "      <td>13.27</td>\n",
       "      <td>3.62</td>\n",
       "      <td>1.24</td>\n",
       "      <td>73.08</td>\n",
       "      <td>0.55</td>\n",
       "      <td>8.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RI     Na    Mg    Al     Si     K    Ca   Ba   Fe  Type\n",
       "0  1.52101  13.64  4.49  1.10  71.78  0.06  8.75  0.0  0.0     1\n",
       "1  1.51761  13.89  3.60  1.36  72.73  0.48  7.83  0.0  0.0     1\n",
       "2  1.51618  13.53  3.55  1.54  72.99  0.39  7.78  0.0  0.0     1\n",
       "3  1.51766  13.21  3.69  1.29  72.61  0.57  8.22  0.0  0.0     1\n",
       "4  1.51742  13.27  3.62  1.24  73.08  0.55  8.07  0.0  0.0     1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glass.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72f945dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RI</th>\n",
       "      <th>Na</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Al</th>\n",
       "      <th>Si</th>\n",
       "      <th>K</th>\n",
       "      <th>Ca</th>\n",
       "      <th>Ba</th>\n",
       "      <th>Fe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.518365</td>\n",
       "      <td>13.407850</td>\n",
       "      <td>2.684533</td>\n",
       "      <td>1.444907</td>\n",
       "      <td>72.650935</td>\n",
       "      <td>0.497056</td>\n",
       "      <td>8.956963</td>\n",
       "      <td>0.175047</td>\n",
       "      <td>0.057009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.003037</td>\n",
       "      <td>0.816604</td>\n",
       "      <td>1.442408</td>\n",
       "      <td>0.499270</td>\n",
       "      <td>0.774546</td>\n",
       "      <td>0.652192</td>\n",
       "      <td>1.423153</td>\n",
       "      <td>0.497219</td>\n",
       "      <td>0.097439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.511150</td>\n",
       "      <td>10.730000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>69.810000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.430000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.516523</td>\n",
       "      <td>12.907500</td>\n",
       "      <td>2.115000</td>\n",
       "      <td>1.190000</td>\n",
       "      <td>72.280000</td>\n",
       "      <td>0.122500</td>\n",
       "      <td>8.240000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.517680</td>\n",
       "      <td>13.300000</td>\n",
       "      <td>3.480000</td>\n",
       "      <td>1.360000</td>\n",
       "      <td>72.790000</td>\n",
       "      <td>0.555000</td>\n",
       "      <td>8.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.519157</td>\n",
       "      <td>13.825000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>1.630000</td>\n",
       "      <td>73.087500</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>9.172500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.533930</td>\n",
       "      <td>17.380000</td>\n",
       "      <td>4.490000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>75.410000</td>\n",
       "      <td>6.210000</td>\n",
       "      <td>16.190000</td>\n",
       "      <td>3.150000</td>\n",
       "      <td>0.510000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               RI          Na          Mg          Al          Si           K  \\\n",
       "count  214.000000  214.000000  214.000000  214.000000  214.000000  214.000000   \n",
       "mean     1.518365   13.407850    2.684533    1.444907   72.650935    0.497056   \n",
       "std      0.003037    0.816604    1.442408    0.499270    0.774546    0.652192   \n",
       "min      1.511150   10.730000    0.000000    0.290000   69.810000    0.000000   \n",
       "25%      1.516523   12.907500    2.115000    1.190000   72.280000    0.122500   \n",
       "50%      1.517680   13.300000    3.480000    1.360000   72.790000    0.555000   \n",
       "75%      1.519157   13.825000    3.600000    1.630000   73.087500    0.610000   \n",
       "max      1.533930   17.380000    4.490000    3.500000   75.410000    6.210000   \n",
       "\n",
       "               Ca          Ba          Fe  \n",
       "count  214.000000  214.000000  214.000000  \n",
       "mean     8.956963    0.175047    0.057009  \n",
       "std      1.423153    0.497219    0.097439  \n",
       "min      5.430000    0.000000    0.000000  \n",
       "25%      8.240000    0.000000    0.000000  \n",
       "50%      8.600000    0.000000    0.000000  \n",
       "75%      9.172500    0.000000    0.100000  \n",
       "max     16.190000    3.150000    0.510000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glass.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c82ce8d",
   "metadata": {},
   "source": [
    "Seleccionamos todas columnas excepto el √≠ndice de refracci√≥n como variables predictoras y la variable RI como variable a predecir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6be76140",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = glass.iloc[:, 0:9]\n",
    "Y = np.ravel(glass.Type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b99d7bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1129d578",
   "metadata": {},
   "source": [
    "En este dataset, sabemos que necesitamos un mayor n√∫mero de √©pocas, de acuerdo a la experiencia en el cuaderno anterior. Por ello, usaremos 200. Probamos a variar la profundidad de la red para un conjunto fijo de neuronas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "73f81c77",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257CFB9F670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D3D395E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D3CB1820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D15419D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D135C790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D3DB7AF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D15411F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D406CDC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D3CB1430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D1541C10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257CFC29310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D135CCA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D3CB1CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257CFC29670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D3CB1310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x00000257CFBC5EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D15418B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D2A1C040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D135C4C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257CFC29280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D12A9C10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000257D1589280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D135CEE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D3CB1040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D1541670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257CFC293A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D12A9550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000257D1541CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "metricas = []\n",
    "modelos = [[64], [64, 64], [64, 64, 64], [64, 64, 64, 64], [64, 64, 64, 64, 64]]\n",
    "for i_modelos in modelos:\n",
    "    metricas.append(regression_model(i_modelos, 9, 5, 200, verbose_=0, seed_=1, optimizer_='rmsprop', metrics_=['mae'], activation_='relu' ))\n",
    "    \n",
    "metricas_profundidad = metricas.copy()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66638a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.9494925737380981, 0.6988823413848877, 0.7761121719215767),\n",
       " (1.159546136856079, 0.7373892068862915, 0.7265820835908385),\n",
       " (1.0407816171646118, 0.7188649773597717, 0.7545864343931489),\n",
       " (0.8756394386291504, 0.6501733660697937, 0.7935265456029338),\n",
       " (1.0193110704421997, 0.6857530474662781, 0.759649149449286)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3291ea8c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D135C160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D3CB1700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D1589E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D5158AF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257CFBC5B80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000257CFC291F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D51588B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D3CB1430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D135C1F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D15411F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257CFBC5B80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000257D135CE50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D12A9EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D3CB1940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D12A9CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D1541D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257CFC29AF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000257D52EE940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D3CB14C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D12A9310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D3CB11F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257CFBC53A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257CFC29670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000257D12A9E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D52623A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D3CB18B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257CFBC5C10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257CFC29430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D16F83A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000257D5262D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257CFC29AF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D12A9040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D1541EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257CFB9F670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D3CB1700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000257D3CB1AF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D5158940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D5165700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D1541430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D5158430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D3CB1550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000257D12A9F70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D3CB1A60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257CFC29700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D1541D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257CFC298B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257CFBC5790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000257D3CB1430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D135CB80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D3DFE700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257CFBC5700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D5158790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D135C550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000257D12A91F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D3DFE310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D3CB11F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257CFBC5CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D135CAF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257CFC29670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000257D40823A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D135C940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D3DFEEE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D12A9670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D3CB1EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D5158310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000257D5158B80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D135C9D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D3CB1F70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257CFBC5F70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257CFC29A60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D1541550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000257D135C8B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "metricas = []\n",
    "neuronas = [32, 64, 128]\n",
    "for n_neurons in neuronas:\n",
    "    modelos = [[n_neurons], [n_neurons, n_neurons], [n_neurons, n_neurons, n_neurons], [n_neurons, n_neurons, n_neurons, n_neurons]]\n",
    "    for i_modelos in modelos:\n",
    "            metricas.append(regression_model(i_modelos, 9, 5, 200, verbose_=0, seed_=1, optimizer_='rmsprop', metrics_=['mae'], activation_='relu' ))\n",
    "\n",
    "metricas_neuronas = metricas.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a5044f15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0040833950042725, 0.772392749786377, 0.763239840941013),\n",
       " (0.7367757558822632, 0.6485392451286316, 0.8262702333888078),\n",
       " (1.754930019378662, 0.8526869416236877, 0.586192206688565),\n",
       " (0.8408551812171936, 0.6348536610603333, 0.8017285713458002),\n",
       " (1.1851806640625, 0.7701775431632996, 0.7205375629854719),\n",
       " (2.1623175144195557, 0.8022724390029907, 0.4901312200306217),\n",
       " (0.9620720148086548, 0.688814103603363, 0.7731459494712412),\n",
       " (1.9493314027786255, 0.8948702812194824, 0.5403528903440159),\n",
       " (0.8724623322486877, 0.6427998542785645, 0.7942756974350498),\n",
       " (1.5520925521850586, 0.7747636437416077, 0.6340206716613652),\n",
       " (0.9826909303665161, 0.6889689564704895, 0.7682840277080227),\n",
       " (1.3658355474472046, 0.7389888167381287, 0.6779395835892658)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metricas_neuronas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8875e282",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_neurons = []\n",
    "col_capas = []\n",
    "\n",
    "neuronas = [32, 64, 128]\n",
    "for n_neurons in neuronas:\n",
    "    modelos = [[n_neurons], [n_neurons, n_neurons], [n_neurons, n_neurons, n_neurons], [n_neurons, n_neurons, n_neurons, n_neurons]]\n",
    "    for i_modelos in modelos:\n",
    "            col_neurons.append(n_neurons)\n",
    "            col_capas.append(len(i_modelos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d43917f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 32  32  32  32  64  64  64  64 128 128 128 128]\n",
      "[1 2 3 4 1 2 3 4 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "col_neurons = np.array(col_neurons)\n",
    "col_capas = np.array(col_capas)\n",
    "print(col_neurons)\n",
    "print(col_capas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "50b3d8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aux1 = pd.DataFrame(metricas_neuronas, columns=['mse', 'mae', 'R2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f775d37f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Capas</th>\n",
       "      <th>Neuronas</th>\n",
       "      <th>mse</th>\n",
       "      <th>mae</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>0.736776</td>\n",
       "      <td>0.648539</td>\n",
       "      <td>0.826270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>0.840855</td>\n",
       "      <td>0.634854</td>\n",
       "      <td>0.801729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>0.872462</td>\n",
       "      <td>0.642800</td>\n",
       "      <td>0.794276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>0.962072</td>\n",
       "      <td>0.688814</td>\n",
       "      <td>0.773146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>128</td>\n",
       "      <td>0.982691</td>\n",
       "      <td>0.688969</td>\n",
       "      <td>0.768284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>1.004083</td>\n",
       "      <td>0.772393</td>\n",
       "      <td>0.763240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>1.185181</td>\n",
       "      <td>0.770178</td>\n",
       "      <td>0.720538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>128</td>\n",
       "      <td>1.365836</td>\n",
       "      <td>0.738989</td>\n",
       "      <td>0.677940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>1.552093</td>\n",
       "      <td>0.774764</td>\n",
       "      <td>0.634021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>1.754930</td>\n",
       "      <td>0.852687</td>\n",
       "      <td>0.586192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>1.949331</td>\n",
       "      <td>0.894870</td>\n",
       "      <td>0.540353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>2.162318</td>\n",
       "      <td>0.802272</td>\n",
       "      <td>0.490131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Capas  Neuronas       mse       mae        R2\n",
       "1       2        32  0.736776  0.648539  0.826270\n",
       "3       4        32  0.840855  0.634854  0.801729\n",
       "8       1       128  0.872462  0.642800  0.794276\n",
       "6       3        64  0.962072  0.688814  0.773146\n",
       "10      3       128  0.982691  0.688969  0.768284\n",
       "0       1        32  1.004083  0.772393  0.763240\n",
       "4       1        64  1.185181  0.770178  0.720538\n",
       "11      4       128  1.365836  0.738989  0.677940\n",
       "9       2       128  1.552093  0.774764  0.634021\n",
       "2       3        32  1.754930  0.852687  0.586192\n",
       "7       4        64  1.949331  0.894870  0.540353\n",
       "5       2        64  2.162318  0.802272  0.490131"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {'Capas': col_capas,\n",
    "        'Neuronas': col_neurons\n",
    "        }\n",
    "df_aux2 = pd.DataFrame(data, columns=['Capas', 'Neuronas'])\n",
    "df_mas_neuronas = pd.concat([df_aux2, df_aux1], axis=1)\n",
    "\n",
    "df_mas_neuronas.sort_values(['mse', 'mae', 'R2' ],\n",
    "                                     ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dcb253",
   "metadata": {},
   "source": [
    "Parece que el mejor coeficiente de correlaci√≥n de pearson se obtiene con dos capas ocultas de 32 neuronas cada una. Sobre este modelo procederemos a probar distintos clasificadores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef955036",
   "metadata": {},
   "source": [
    "### Par√°metros de optimizaci√≥n\n",
    "\n",
    "Usando la mejor configuraci√≥n anterior, es decir, 2 capas con 32 neuronas, probaremos distintos optimizadores. Cambiamos el optimizador RMSprop por sgd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a677ce65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop, SGD\n",
    "lr = [0.1, 0.01, 0.001, 0.0001]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f4e00c",
   "metadata": {},
   "source": [
    "### RMSprop con varios valores de learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cbbab9bb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D3DFE9D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D5158A60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D51D7CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D78BC790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D51D75E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000257D12A9670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D3CB1790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257CFBC5940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D1589160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D3DFEEE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D1589280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000257CFBC5940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D135C550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D671D040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D5158D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D135C8B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D3CB1CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000257D12A9670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D1541700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257CFBC5B80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257CFC29E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D3DFE9D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D51D7700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000257D5158940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "metricas = []\n",
    "for lr_i in lr:\n",
    "    rmsprop = RMSprop(lr= lr_i)\n",
    "    metricas.append(regression_model([32, 32], 9, 5, 200, verbose_=0, seed_=1, optimizer_=rmsprop, metrics_=['mae'], activation_='relu' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e754bb75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3.222461223602295, 0.9445854425430298, 0.24015221056219804),\n",
       " (1.0601695775985718, 0.7264225482940674, 0.7500148251876033),\n",
       " (1.4421206712722778, 0.7452108860015869, 0.6599517308206171),\n",
       " (1.5500553846359253, 0.9527155756950378, 0.6345010312878121)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metricas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3cbfa1",
   "metadata": {},
   "source": [
    "### SGD con varios valores de learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4f2260f3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257CFC294C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D671DE50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257CFBC5EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D51D7820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D29BD040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000257D29BDEE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D51589D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D135C550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257CFBC5A60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D671D820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D1589790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000257D3CB1040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D50F73A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D3CB11F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D671DEE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D135CCA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D12A9820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000257D1541160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257CFBC5C10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D7AFAEE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D15414C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257CFBC51F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000257D12A9700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000257D12A9790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "metricas2 = []\n",
    "for lr_i in lr:\n",
    "    sgd = SGD(lr= lr_i)\n",
    "    metricas2.append(regression_model([32, 32], 9, 5, 40, verbose_=0, seed_=1, optimizer_=sgd, metrics_=['mae'], activation_='relu' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3c9fb77d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2.210137128829956, 0.9572741985321045, 0.4788555325797056),\n",
       " (1.6006039381027222, 0.9083406329154968, 0.6225818288659197),\n",
       " (2.280722141265869, 1.063818335533142, 0.4622117440772856),\n",
       " (9.183472633361816, 2.242046594619751, -1.1654386401245453)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metricas2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "46c35351",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate([lr, lr])\n",
    "methods = np.concatenate( [[\"RMSProp\"]*4, ['SGD']*4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0e080b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index      lr Optimizer  index       mse       mae        R2\n",
      "0      0  0.1000   RMSProp      0  3.222461  0.944585  0.240152\n",
      "1      1  0.0100   RMSProp      1  1.060170  0.726423  0.750015\n",
      "2      2  0.0010   RMSProp      2  1.442121  0.745211  0.659952\n",
      "3      3  0.0001   RMSProp      3  1.550055  0.952716  0.634501\n",
      "4      4  0.1000       SGD      0  2.210137  0.957274  0.478856\n",
      "5      5  0.0100       SGD      1  1.600604  0.908341  0.622582\n",
      "6      6  0.0010       SGD      2  2.280722  1.063818  0.462212\n",
      "7      7  0.0001       SGD      3  9.183473  2.242047 -1.165439\n"
     ]
    }
   ],
   "source": [
    "df_aux1 = pd.DataFrame(metricas, columns=['mse', 'mae', 'R2'])\n",
    "df_aux2 = pd.DataFrame(metricas2, columns=['mse', 'mae', 'R2'])\n",
    "data = {'lr': np.concatenate([lr,lr]),\n",
    "        'Optimizer': methods\n",
    "        }\n",
    "df_aux3 = pd.DataFrame(data, columns=['lr', 'Optimizer'])\n",
    "df_aux4 = pd.concat([df_aux1, df_aux2], axis=0)\n",
    "\n",
    "df_optimizers = pd.concat([df_aux3.reset_index(), df_aux4.reset_index()], axis=1, sort=False)\n",
    "print(df_optimizers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca86235c",
   "metadata": {},
   "source": [
    "Vemos que en el caso del optimizador RMSProp, el valor 0.001 proporciona el mejor resultado, aunque es muy parecido al obtenido con 0.0001. En cambio, para SGD, el valor √≥ptimo es 0.01."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf5ba20",
   "metadata": {},
   "source": [
    "### Conclusiones\n",
    "\n",
    "En este caso, mediante regresi√≥n, hemos obtenido un modelo de dos capas ocultas con 64 unidades ocultas que ofrece un $R^2$ de 0.826270, $mse$  0.736776 y $mae$ 0.6485390. Dado el tama√±o de nuestro dataset, 214 muestras, tiene sentido que no sean necesarios modelos m√°s grandes para ofrecer un desempe√±o *decente*. No conseguimos valores superiores al obtenido. De los clasificadores probados, los par√°metros por defecto del optimizador *RMSProp* ofrecen mejores resultados que variaciones en el *learning rate* del mismo y del optimizador descenso por gradiente estoc√°stico."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46c380b",
   "metadata": {},
   "source": [
    "# Clasificaci√≥n del riesgo de abandono de los clientes de un banco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0212915e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "053752f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3dd9319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5b5492e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"Churn_Modelling.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "de618f1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0       2       0.00              1          1               1   \n",
       "1       1   83807.86              1          0               1   \n",
       "2       8  159660.80              3          1               0   \n",
       "3       1       0.00              2          0               0   \n",
       "4       2  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "77073e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, 3:13].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "177593fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[619, 'France', 'Female', 42, 2, 0.0, 1, 1, 1, 101348.88],\n",
       "       [608, 'Spain', 'Female', 41, 1, 83807.86, 1, 0, 1, 112542.58],\n",
       "       [502, 'France', 'Female', 42, 8, 159660.8, 3, 1, 0, 113931.57],\n",
       "       [699, 'France', 'Female', 39, 1, 0.0, 2, 0, 0, 93826.63]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "eeb7ed23",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataset.iloc[:, 13].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5c7f22f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4074d9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder_X_1 = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2e4b467e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9ebec85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder_X_2 = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0c630c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2327462c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "35fc78f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b7cfb6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "617f6697",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ed0aaf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sc.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3ec52af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "70d9c694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "eac6f564",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3957ea44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "14f4ba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c72f789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(6, activation='relu', input_shape=(10,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3908cb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(6, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "66831e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "813067f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "019c615d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 4s 550us/step - loss: 0.4719 - accuracy: 0.7930\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 4s 553us/step - loss: 0.4280 - accuracy: 0.8124\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 4s 545us/step - loss: 0.4064 - accuracy: 0.8296\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 4s 561us/step - loss: 0.3767 - accuracy: 0.8462\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 5s 610us/step - loss: 0.3638 - accuracy: 0.8544\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 4s 544us/step - loss: 0.3559 - accuracy: 0.8562\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 4s 551us/step - loss: 0.3529 - accuracy: 0.8576\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 5s 566us/step - loss: 0.3500 - accuracy: 0.8585\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 5s 579us/step - loss: 0.3476 - accuracy: 0.8580\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 5s 564us/step - loss: 0.3462 - accuracy: 0.8604\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 5s 584us/step - loss: 0.3471 - accuracy: 0.85850s - loss: 0.3460 - accura\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 5s 578us/step - loss: 0.3459 - accuracy: 0.8596\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 5s 588us/step - loss: 0.3442 - accuracy: 0.86020s\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 4s 553us/step - loss: 0.3438 - accuracy: 0.8600\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 4s 558us/step - loss: 0.3426 - accuracy: 0.8626\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 5s 635us/step - loss: 0.3427 - accuracy: 0.8595\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 5s 603us/step - loss: 0.3427 - accuracy: 0.8614\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 5s 580us/step - loss: 0.3418 - accuracy: 0.8620\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 5s 572us/step - loss: 0.3417 - accuracy: 0.8612\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 5s 609us/step - loss: 0.3401 - accuracy: 0.8621\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 4s 555us/step - loss: 0.3411 - accuracy: 0.8612\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 5s 600us/step - loss: 0.3400 - accuracy: 0.8606\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 5s 591us/step - loss: 0.3403 - accuracy: 0.8606\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 4s 562us/step - loss: 0.3397 - accuracy: 0.8612\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 5s 563us/step - loss: 0.3398 - accuracy: 0.8624\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 4s 541us/step - loss: 0.3394 - accuracy: 0.8602\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 4s 540us/step - loss: 0.3389 - accuracy: 0.8606\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 5s 572us/step - loss: 0.3384 - accuracy: 0.8610\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 5s 564us/step - loss: 0.3380 - accuracy: 0.8618\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 5s 594us/step - loss: 0.3382 - accuracy: 0.8634\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 5s 663us/step - loss: 0.3379 - accuracy: 0.8622\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 5s 597us/step - loss: 0.3376 - accuracy: 0.8609\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 5s 680us/step - loss: 0.3372 - accuracy: 0.8630\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 5s 597us/step - loss: 0.3379 - accuracy: 0.8616\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 6s 767us/step - loss: 0.3372 - accuracy: 0.8622\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 5s 580us/step - loss: 0.3367 - accuracy: 0.8637\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 4s 533us/step - loss: 0.3366 - accuracy: 0.8606\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 4s 558us/step - loss: 0.3366 - accuracy: 0.8614\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 4s 535us/step - loss: 0.3365 - accuracy: 0.8633\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 4s 536us/step - loss: 0.3364 - accuracy: 0.8609\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 4s 554us/step - loss: 0.3358 - accuracy: 0.8610\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 7s 862us/step - loss: 0.3364 - accuracy: 0.8591\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 0.3354 - accuracy: 0.8616\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 0.3351 - accuracy: 0.8619\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 7s 855us/step - loss: 0.3359 - accuracy: 0.8597\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 6s 775us/step - loss: 0.3363 - accuracy: 0.8618\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 7s 894us/step - loss: 0.3356 - accuracy: 0.8641\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 5s 646us/step - loss: 0.3355 - accuracy: 0.8600\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 5s 637us/step - loss: 0.3354 - accuracy: 0.8627\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 6s 753us/step - loss: 0.3344 - accuracy: 0.8629\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 4s 545us/step - loss: 0.3352 - accuracy: 0.8635\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 4s 530us/step - loss: 0.3341 - accuracy: 0.8639\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 5s 623us/step - loss: 0.3343 - accuracy: 0.8614\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 4s 553us/step - loss: 0.3332 - accuracy: 0.8629\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 5s 674us/step - loss: 0.3340 - accuracy: 0.8633\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 6s 690us/step - loss: 0.3331 - accuracy: 0.8630\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 6s 706us/step - loss: 0.3328 - accuracy: 0.8640\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 5s 623us/step - loss: 0.3331 - accuracy: 0.8600\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 5s 575us/step - loss: 0.3328 - accuracy: 0.8624\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 5s 602us/step - loss: 0.3330 - accuracy: 0.8616\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 5s 611us/step - loss: 0.3325 - accuracy: 0.86180s - loss: 0.332\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 5s 566us/step - loss: 0.3319 - accuracy: 0.8627\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 0.3315 - accuracy: 0.86 - 5s 570us/step - loss: 0.3314 - accuracy: 0.8614\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 5s 566us/step - loss: 0.3316 - accuracy: 0.8622\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 5s 593us/step - loss: 0.3316 - accuracy: 0.8621\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 5s 571us/step - loss: 0.3315 - accuracy: 0.8633\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 5s 604us/step - loss: 0.3308 - accuracy: 0.8608\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 6s 689us/step - loss: 0.3315 - accuracy: 0.8624\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 5s 631us/step - loss: 0.3307 - accuracy: 0.8631\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 5s 663us/step - loss: 0.3312 - accuracy: 0.8622\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 5s 645us/step - loss: 0.3309 - accuracy: 0.8643\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 5s 652us/step - loss: 0.3310 - accuracy: 0.8637\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 4s 549us/step - loss: 0.3312 - accuracy: 0.8627\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 5s 585us/step - loss: 0.3303 - accuracy: 0.8631\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 5s 567us/step - loss: 0.3312 - accuracy: 0.8641\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 5s 568us/step - loss: 0.3308 - accuracy: 0.8630\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 5s 580us/step - loss: 0.3305 - accuracy: 0.8622\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 5s 563us/step - loss: 0.3301 - accuracy: 0.8637\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 5s 565us/step - loss: 0.3310 - accuracy: 0.8635\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 5s 581us/step - loss: 0.3307 - accuracy: 0.8633\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 5s 608us/step - loss: 0.3306 - accuracy: 0.8624\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 5s 593us/step - loss: 0.3299 - accuracy: 0.8627\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 6s 786us/step - loss: 0.3305 - accuracy: 0.8640\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 5s 648us/step - loss: 0.3308 - accuracy: 0.8622\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 5s 568us/step - loss: 0.3308 - accuracy: 0.8627\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 5s 586us/step - loss: 0.3311 - accuracy: 0.8645\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 5s 583us/step - loss: 0.3303 - accuracy: 0.8619\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 4s 555us/step - loss: 0.3305 - accuracy: 0.8639\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 5s 584us/step - loss: 0.3297 - accuracy: 0.8643\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 5s 567us/step - loss: 0.3302 - accuracy: 0.8649\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 5s 575us/step - loss: 0.3296 - accuracy: 0.8639\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 4s 560us/step - loss: 0.3308 - accuracy: 0.8634\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 4s 554us/step - loss: 0.3301 - accuracy: 0.8636\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 5s 565us/step - loss: 0.3300 - accuracy: 0.8635\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 5s 583us/step - loss: 0.3298 - accuracy: 0.8640\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 5s 579us/step - loss: 0.3306 - accuracy: 0.8627\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 5s 574us/step - loss: 0.3305 - accuracy: 0.8625\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 4s 558us/step - loss: 0.3303 - accuracy: 0.8650\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 5s 606us/step - loss: 0.3304 - accuracy: 0.8633\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 4s 545us/step - loss: 0.3292 - accuracy: 0.8651\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x257d2801790>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train, epochs=100, batch_size=1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9968e61f",
   "metadata": {},
   "source": [
    "Una de las primeras observaciones que encontramos es la cantidad de tiempo que necesita tensorflow para ejecutar todas las √©pocas ya que estamos usando un conjunto *batch* de 1 elemento. Es decir, el modelo tiene que realizar la actualizaci√≥n de los pesos usando el resultado proveniente de haber obtenido resultados con un elemento del conjunto de entrenamiento. Evidentemente, este m√©todo no es el m√°s r√°pido, de ah√≠ los m√©todos en tandas. Para m√°s inri, si se consigue usar una gpu dedicada, el uso de t√©cnicas en tanda acelera el proceso ya que permite realizar muchos c√°lculos en paralelo. En estas pr√°cticas no estamos usando gpu dedicada ya que realizamos los cuadernos en windows, en el cual la instalaci√≥n y configuraci√≥n de jupyter notebook con CUDA es mucho m√°s complicada que en Linux, donde s√≠ es f√°cil de ejecutar. La ganancia en tiempo obtenida es notable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "824e02f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "#y_pred = (y_pred > 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "69b88477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.864"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_test.reshape(1,-1) == np.rint(y_pred.reshape(1, -1)))/len(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4a0b2f",
   "metadata": {},
   "source": [
    "Obtenemos un *accuracy* de 0.864. De acuerdo al anterior p√°rrafo, es interesante ver si las mismas √©pocas usando un n√∫mero de batch mayor empeora el rendimiento a nivel precisi√≥n a pesar de ganar en tiempo de ejecuci√≥n. Probamos con un tama√±o de tanda = 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "76339af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_2 = classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9fa227a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3221 - accuracy: 0.8658\n",
      "Epoch 2/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3218 - accuracy: 0.8654\n",
      "Epoch 3/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3221 - accuracy: 0.8650\n",
      "Epoch 4/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3220 - accuracy: 0.8648\n",
      "Epoch 5/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3220 - accuracy: 0.8658\n",
      "Epoch 6/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3220 - accuracy: 0.8650\n",
      "Epoch 7/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3219 - accuracy: 0.8655\n",
      "Epoch 8/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3219 - accuracy: 0.8650\n",
      "Epoch 9/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3220 - accuracy: 0.8656\n",
      "Epoch 10/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3218 - accuracy: 0.8660\n",
      "Epoch 11/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3218 - accuracy: 0.8654\n",
      "Epoch 12/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3219 - accuracy: 0.8641\n",
      "Epoch 13/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3222 - accuracy: 0.8660\n",
      "Epoch 14/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3219 - accuracy: 0.8660\n",
      "Epoch 15/100\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3222 - accuracy: 0.8639\n",
      "Epoch 16/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3219 - accuracy: 0.8655\n",
      "Epoch 17/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3217 - accuracy: 0.8654\n",
      "Epoch 18/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3219 - accuracy: 0.8655\n",
      "Epoch 19/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3220 - accuracy: 0.8646\n",
      "Epoch 20/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3218 - accuracy: 0.8658\n",
      "Epoch 21/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3218 - accuracy: 0.8644\n",
      "Epoch 22/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3218 - accuracy: 0.8652\n",
      "Epoch 23/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3219 - accuracy: 0.8656\n",
      "Epoch 24/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3220 - accuracy: 0.8655\n",
      "Epoch 25/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3217 - accuracy: 0.8655\n",
      "Epoch 26/100\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3219 - accuracy: 0.8652\n",
      "Epoch 27/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3220 - accuracy: 0.8651\n",
      "Epoch 28/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3222 - accuracy: 0.8652\n",
      "Epoch 29/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3218 - accuracy: 0.8654\n",
      "Epoch 30/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3219 - accuracy: 0.8649\n",
      "Epoch 31/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3220 - accuracy: 0.8654\n",
      "Epoch 32/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3218 - accuracy: 0.8651\n",
      "Epoch 33/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3219 - accuracy: 0.8645\n",
      "Epoch 34/100\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3219 - accuracy: 0.8652\n",
      "Epoch 35/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3220 - accuracy: 0.8646\n",
      "Epoch 36/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3219 - accuracy: 0.8654\n",
      "Epoch 37/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3217 - accuracy: 0.8651\n",
      "Epoch 38/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3217 - accuracy: 0.8650\n",
      "Epoch 39/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3217 - accuracy: 0.8651\n",
      "Epoch 40/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3218 - accuracy: 0.8662\n",
      "Epoch 41/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3218 - accuracy: 0.8646\n",
      "Epoch 42/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3218 - accuracy: 0.8652\n",
      "Epoch 43/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3217 - accuracy: 0.8652\n",
      "Epoch 44/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3218 - accuracy: 0.8658\n",
      "Epoch 45/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3218 - accuracy: 0.8658\n",
      "Epoch 46/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3218 - accuracy: 0.8650\n",
      "Epoch 47/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3221 - accuracy: 0.8655\n",
      "Epoch 48/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3218 - accuracy: 0.8656\n",
      "Epoch 49/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3218 - accuracy: 0.8658\n",
      "Epoch 50/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8651\n",
      "Epoch 51/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3218 - accuracy: 0.8654\n",
      "Epoch 52/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3217 - accuracy: 0.8652\n",
      "Epoch 53/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8662\n",
      "Epoch 54/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8654\n",
      "Epoch 55/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3217 - accuracy: 0.8646\n",
      "Epoch 56/100\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3218 - accuracy: 0.8651\n",
      "Epoch 57/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3217 - accuracy: 0.8660\n",
      "Epoch 58/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8658\n",
      "Epoch 59/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3218 - accuracy: 0.8654\n",
      "Epoch 60/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3218 - accuracy: 0.8652\n",
      "Epoch 61/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3218 - accuracy: 0.8652\n",
      "Epoch 62/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8660\n",
      "Epoch 63/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3217 - accuracy: 0.8660\n",
      "Epoch 64/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8655\n",
      "Epoch 65/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3217 - accuracy: 0.8660\n",
      "Epoch 66/100\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3217 - accuracy: 0.8648\n",
      "Epoch 67/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3217 - accuracy: 0.8659\n",
      "Epoch 68/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3218 - accuracy: 0.8652\n",
      "Epoch 69/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8658\n",
      "Epoch 70/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3220 - accuracy: 0.8650\n",
      "Epoch 71/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8656\n",
      "Epoch 72/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8648\n",
      "Epoch 73/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8662\n",
      "Epoch 74/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3217 - accuracy: 0.8658\n",
      "Epoch 75/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3217 - accuracy: 0.8644\n",
      "Epoch 76/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3217 - accuracy: 0.8651\n",
      "Epoch 77/100\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3216 - accuracy: 0.8649\n",
      "Epoch 78/100\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3213 - accuracy: 0.8661\n",
      "Epoch 79/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3218 - accuracy: 0.8662\n",
      "Epoch 80/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3217 - accuracy: 0.8656\n",
      "Epoch 81/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8660\n",
      "Epoch 82/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8655\n",
      "Epoch 83/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8659\n",
      "Epoch 84/100\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3218 - accuracy: 0.8661\n",
      "Epoch 85/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8658\n",
      "Epoch 86/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3218 - accuracy: 0.8661\n",
      "Epoch 87/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8655\n",
      "Epoch 88/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8659\n",
      "Epoch 89/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3217 - accuracy: 0.8656\n",
      "Epoch 90/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3217 - accuracy: 0.8661\n",
      "Epoch 91/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8660\n",
      "Epoch 92/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8655\n",
      "Epoch 93/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8656\n",
      "Epoch 94/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8648\n",
      "Epoch 95/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3217 - accuracy: 0.8659\n",
      "Epoch 96/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3217 - accuracy: 0.8665\n",
      "Epoch 97/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3218 - accuracy: 0.8659\n",
      "Epoch 98/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8674\n",
      "Epoch 99/100\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3216 - accuracy: 0.8656\n",
      "Epoch 100/100\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3217 - accuracy: 0.8662\n"
     ]
    }
   ],
   "source": [
    "prueba_2 = classifier_2.fit(X_train, y_train, epochs=100, batch_size=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "53f740d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.867"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = classifier_2.predict(X_test)\n",
    "np.sum(y_test.reshape(1,-1) == np.rint(y_pred.reshape(1, -1)))/len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f905d1af",
   "metadata": {},
   "source": [
    "Obtenemos la misma puntuaci√≥n que en el caso anterior con un tiempo de ejecuci√≥n mucho menor (segundos frente a 2-3 minutos en mi pc en la prueba anterior). Por lo tanto, actualizar los pesos mediante la media de los gradientes de los distintos gradientes obtenidos con cada uno de los elementos del batch provee buenos resultados a la vez que mejoramos los tiempos. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0281de51",
   "metadata": {},
   "source": [
    "Probamos ahora a aumentar el n√∫mero de iteraciones. Antes de ejecutar una nueva prueba, podemos intuir el resultado de aumentar las iteraciones viendo la evoluci√≥n de *accuracy* durante el entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "47d6ba74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x257db211af0>]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(prueba_2.history['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46085d7",
   "metadata": {},
   "source": [
    "Como podemos observar, desde el principio obtenemos un valor de 0.86 y durante todo el entrenamiento no obtenemos un gran aumento (fij√©monos en el rango del eje y). Podemos intuir que aumentando el n√∫mero de √©pocas, no obtengamos muchos mejores resultados ya que el modelo parece *saturar* desde el primer momento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "89787074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3218 - accuracy: 0.8658\n",
      "Epoch 2/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8652\n",
      "Epoch 3/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8654\n",
      "Epoch 4/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8658\n",
      "Epoch 5/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8651\n",
      "Epoch 6/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8655\n",
      "Epoch 7/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8659\n",
      "Epoch 8/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8658\n",
      "Epoch 9/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8660\n",
      "Epoch 10/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3217 - accuracy: 0.8656\n",
      "Epoch 11/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8652\n",
      "Epoch 12/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3217 - accuracy: 0.8659\n",
      "Epoch 13/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8661\n",
      "Epoch 14/300\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3216 - accuracy: 0.8656\n",
      "Epoch 15/300\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3215 - accuracy: 0.8649\n",
      "Epoch 16/300\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3217 - accuracy: 0.8660\n",
      "Epoch 17/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8654\n",
      "Epoch 18/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8659\n",
      "Epoch 19/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8656\n",
      "Epoch 20/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3217 - accuracy: 0.8660\n",
      "Epoch 21/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8664\n",
      "Epoch 22/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3217 - accuracy: 0.8660\n",
      "Epoch 23/300\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3215 - accuracy: 0.8659\n",
      "Epoch 24/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3217 - accuracy: 0.8649\n",
      "Epoch 25/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8658\n",
      "Epoch 26/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3217 - accuracy: 0.8660\n",
      "Epoch 27/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8654\n",
      "Epoch 28/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8659\n",
      "Epoch 29/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8655\n",
      "Epoch 30/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8651\n",
      "Epoch 31/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8655\n",
      "Epoch 32/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8662\n",
      "Epoch 33/300\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3215 - accuracy: 0.8658\n",
      "Epoch 34/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8655\n",
      "Epoch 35/300\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3214 - accuracy: 0.8656\n",
      "Epoch 36/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8656\n",
      "Epoch 37/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8665\n",
      "Epoch 38/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8661\n",
      "Epoch 39/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3217 - accuracy: 0.8656\n",
      "Epoch 40/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8656\n",
      "Epoch 41/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3217 - accuracy: 0.8660\n",
      "Epoch 42/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8651\n",
      "Epoch 43/300\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3216 - accuracy: 0.8656\n",
      "Epoch 44/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8668\n",
      "Epoch 45/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8656\n",
      "Epoch 46/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8662\n",
      "Epoch 47/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8649\n",
      "Epoch 48/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8660\n",
      "Epoch 49/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8664\n",
      "Epoch 50/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8648\n",
      "Epoch 51/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8659\n",
      "Epoch 52/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8655\n",
      "Epoch 53/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8652\n",
      "Epoch 54/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8656\n",
      "Epoch 55/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8660\n",
      "Epoch 56/300\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.3276 - accuracy: 0.86 - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8649\n",
      "Epoch 57/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8658\n",
      "Epoch 58/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8660\n",
      "Epoch 59/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8654\n",
      "Epoch 60/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8658\n",
      "Epoch 61/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8650\n",
      "Epoch 62/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8651\n",
      "Epoch 63/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8660\n",
      "Epoch 64/300\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3216 - accuracy: 0.8654\n",
      "Epoch 65/300\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3215 - accuracy: 0.8655\n",
      "Epoch 66/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8659\n",
      "Epoch 67/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8648\n",
      "Epoch 68/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8659\n",
      "Epoch 69/300\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3215 - accuracy: 0.8654\n",
      "Epoch 70/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8660\n",
      "Epoch 71/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8650\n",
      "Epoch 72/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8661\n",
      "Epoch 73/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8659\n",
      "Epoch 74/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8654\n",
      "Epoch 75/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8658\n",
      "Epoch 76/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8659\n",
      "Epoch 77/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8650\n",
      "Epoch 78/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8655\n",
      "Epoch 79/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8665\n",
      "Epoch 80/300\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3214 - accuracy: 0.8660\n",
      "Epoch 81/300\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3216 - accuracy: 0.8659\n",
      "Epoch 82/300\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3216 - accuracy: 0.8661\n",
      "Epoch 83/300\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3216 - accuracy: 0.8654\n",
      "Epoch 84/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8658\n",
      "Epoch 85/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8649\n",
      "Epoch 86/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8650\n",
      "Epoch 87/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8661\n",
      "Epoch 88/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8659\n",
      "Epoch 89/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8655\n",
      "Epoch 90/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8646\n",
      "Epoch 91/300\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3214 - accuracy: 0.8666\n",
      "Epoch 92/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8658\n",
      "Epoch 93/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8658\n",
      "Epoch 94/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8652\n",
      "Epoch 95/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8659\n",
      "Epoch 96/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8652\n",
      "Epoch 97/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8652\n",
      "Epoch 98/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8661\n",
      "Epoch 99/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8656\n",
      "Epoch 100/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8660\n",
      "Epoch 101/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8656\n",
      "Epoch 102/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8660\n",
      "Epoch 103/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8655\n",
      "Epoch 104/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8656\n",
      "Epoch 105/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8648\n",
      "Epoch 106/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8656\n",
      "Epoch 107/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8652\n",
      "Epoch 108/300\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3214 - accuracy: 0.8656\n",
      "Epoch 109/300\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3216 - accuracy: 0.8659\n",
      "Epoch 110/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8652\n",
      "Epoch 111/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8652\n",
      "Epoch 112/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8650\n",
      "Epoch 113/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8656\n",
      "Epoch 114/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8658\n",
      "Epoch 115/300\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3213 - accuracy: 0.8652\n",
      "Epoch 116/300\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3212 - accuracy: 0.8666\n",
      "Epoch 117/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8656\n",
      "Epoch 118/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8655\n",
      "Epoch 119/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8650\n",
      "Epoch 120/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8666\n",
      "Epoch 121/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8656\n",
      "Epoch 122/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8655\n",
      "Epoch 123/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3217 - accuracy: 0.8658\n",
      "Epoch 124/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8660\n",
      "Epoch 125/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8655\n",
      "Epoch 126/300\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3214 - accuracy: 0.8650\n",
      "Epoch 127/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8656\n",
      "Epoch 128/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3217 - accuracy: 0.8662\n",
      "Epoch 129/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8658\n",
      "Epoch 130/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8659\n",
      "Epoch 131/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8661\n",
      "Epoch 132/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8656\n",
      "Epoch 133/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8656\n",
      "Epoch 134/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3212 - accuracy: 0.8655\n",
      "Epoch 135/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8658\n",
      "Epoch 136/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8650\n",
      "Epoch 137/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8656\n",
      "Epoch 138/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8656\n",
      "Epoch 139/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8664\n",
      "Epoch 140/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8662\n",
      "Epoch 141/300\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3215 - accuracy: 0.8654\n",
      "Epoch 142/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8664\n",
      "Epoch 143/300\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3213 - accuracy: 0.8659\n",
      "Epoch 144/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8654\n",
      "Epoch 145/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8659\n",
      "Epoch 146/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3212 - accuracy: 0.8656\n",
      "Epoch 147/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8650\n",
      "Epoch 148/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8662\n",
      "Epoch 149/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8658\n",
      "Epoch 150/300\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3216 - accuracy: 0.8648\n",
      "Epoch 151/300\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3214 - accuracy: 0.8656\n",
      "Epoch 152/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8648\n",
      "Epoch 153/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8654\n",
      "Epoch 154/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8654\n",
      "Epoch 155/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8655: 0s - loss: 0.3252 - accuracy: 0.86\n",
      "Epoch 156/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3217 - accuracy: 0.8654\n",
      "Epoch 157/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8666\n",
      "Epoch 158/300\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3213 - accuracy: 0.8658\n",
      "Epoch 159/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8658\n",
      "Epoch 160/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8648\n",
      "Epoch 161/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8660\n",
      "Epoch 162/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3212 - accuracy: 0.8658\n",
      "Epoch 163/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8655\n",
      "Epoch 164/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8654\n",
      "Epoch 165/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8656\n",
      "Epoch 166/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8649\n",
      "Epoch 167/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3217 - accuracy: 0.8659\n",
      "Epoch 168/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8661\n",
      "Epoch 169/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8660\n",
      "Epoch 170/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8655\n",
      "Epoch 171/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8656\n",
      "Epoch 172/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8655\n",
      "Epoch 173/300\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3213 - accuracy: 0.8665\n",
      "Epoch 174/300\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3212 - accuracy: 0.8656\n",
      "Epoch 175/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8654\n",
      "Epoch 176/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8662\n",
      "Epoch 177/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8656\n",
      "Epoch 178/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3212 - accuracy: 0.8656\n",
      "Epoch 179/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8666\n",
      "Epoch 180/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8664\n",
      "Epoch 181/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8661\n",
      "Epoch 182/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8650\n",
      "Epoch 183/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8660\n",
      "Epoch 184/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8661\n",
      "Epoch 185/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8656\n",
      "Epoch 186/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8658\n",
      "Epoch 187/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3212 - accuracy: 0.8655\n",
      "Epoch 188/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8660\n",
      "Epoch 189/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8650\n",
      "Epoch 190/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8651\n",
      "Epoch 191/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8659\n",
      "Epoch 192/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8655\n",
      "Epoch 193/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8656\n",
      "Epoch 194/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8659\n",
      "Epoch 195/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8646\n",
      "Epoch 196/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8652\n",
      "Epoch 197/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3211 - accuracy: 0.8659\n",
      "Epoch 198/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3212 - accuracy: 0.8650\n",
      "Epoch 199/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8652\n",
      "Epoch 200/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8649\n",
      "Epoch 201/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8659\n",
      "Epoch 202/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8652\n",
      "Epoch 203/300\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3212 - accuracy: 0.8668\n",
      "Epoch 204/300\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3214 - accuracy: 0.8656\n",
      "Epoch 205/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3212 - accuracy: 0.8666\n",
      "Epoch 206/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8659\n",
      "Epoch 207/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8658\n",
      "Epoch 208/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8658\n",
      "Epoch 209/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8649\n",
      "Epoch 210/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3212 - accuracy: 0.8652\n",
      "Epoch 211/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3212 - accuracy: 0.8661\n",
      "Epoch 212/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8644\n",
      "Epoch 213/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8661\n",
      "Epoch 214/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8660\n",
      "Epoch 215/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8654\n",
      "Epoch 216/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8655\n",
      "Epoch 217/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8650\n",
      "Epoch 218/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8662\n",
      "Epoch 219/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8655\n",
      "Epoch 220/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8656\n",
      "Epoch 221/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8651\n",
      "Epoch 222/300\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3212 - accuracy: 0.8654\n",
      "Epoch 223/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8655\n",
      "Epoch 224/300\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3213 - accuracy: 0.8651\n",
      "Epoch 225/300\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3214 - accuracy: 0.8646\n",
      "Epoch 226/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8662\n",
      "Epoch 227/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8659\n",
      "Epoch 228/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8659\n",
      "Epoch 229/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8664\n",
      "Epoch 230/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8652\n",
      "Epoch 231/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8654\n",
      "Epoch 232/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8660\n",
      "Epoch 233/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8660\n",
      "Epoch 234/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8654\n",
      "Epoch 235/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8658\n",
      "Epoch 236/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8662\n",
      "Epoch 237/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8651\n",
      "Epoch 238/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3212 - accuracy: 0.8654\n",
      "Epoch 239/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8646\n",
      "Epoch 240/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3212 - accuracy: 0.8654\n",
      "Epoch 241/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8664\n",
      "Epoch 242/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8655\n",
      "Epoch 243/300\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3215 - accuracy: 0.8659\n",
      "Epoch 244/300\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3214 - accuracy: 0.8654\n",
      "Epoch 245/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8656\n",
      "Epoch 246/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3212 - accuracy: 0.8658\n",
      "Epoch 247/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8660\n",
      "Epoch 248/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8651\n",
      "Epoch 249/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3212 - accuracy: 0.8652\n",
      "Epoch 250/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8656\n",
      "Epoch 251/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8660\n",
      "Epoch 252/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8660\n",
      "Epoch 253/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8652\n",
      "Epoch 254/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3212 - accuracy: 0.8655\n",
      "Epoch 255/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8654\n",
      "Epoch 256/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8655\n",
      "Epoch 257/300\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3213 - accuracy: 0.8658\n",
      "Epoch 258/300\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3216 - accuracy: 0.8643\n",
      "Epoch 259/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3211 - accuracy: 0.8665\n",
      "Epoch 260/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3212 - accuracy: 0.8662\n",
      "Epoch 261/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8655\n",
      "Epoch 262/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3212 - accuracy: 0.8654\n",
      "Epoch 263/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3211 - accuracy: 0.8656\n",
      "Epoch 264/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8655\n",
      "Epoch 265/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8654\n",
      "Epoch 266/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8654\n",
      "Epoch 267/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8662\n",
      "Epoch 268/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8654\n",
      "Epoch 269/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8652\n",
      "Epoch 270/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3211 - accuracy: 0.8658\n",
      "Epoch 271/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8662\n",
      "Epoch 272/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3212 - accuracy: 0.8665\n",
      "Epoch 273/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3212 - accuracy: 0.8658\n",
      "Epoch 274/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8654\n",
      "Epoch 275/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3212 - accuracy: 0.8655\n",
      "Epoch 276/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3212 - accuracy: 0.8654\n",
      "Epoch 277/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8658\n",
      "Epoch 278/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3212 - accuracy: 0.8660\n",
      "Epoch 279/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3212 - accuracy: 0.8664\n",
      "Epoch 280/300\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3214 - accuracy: 0.8666\n",
      "Epoch 281/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3211 - accuracy: 0.8654\n",
      "Epoch 282/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8650\n",
      "Epoch 283/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8660\n",
      "Epoch 284/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8655\n",
      "Epoch 285/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8651\n",
      "Epoch 286/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3212 - accuracy: 0.8645\n",
      "Epoch 287/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8661\n",
      "Epoch 288/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8660\n",
      "Epoch 289/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3212 - accuracy: 0.8652\n",
      "Epoch 290/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8662\n",
      "Epoch 291/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8644\n",
      "Epoch 292/300\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3212 - accuracy: 0.8659\n",
      "Epoch 293/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8655\n",
      "Epoch 294/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3212 - accuracy: 0.8658\n",
      "Epoch 295/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3212 - accuracy: 0.8652\n",
      "Epoch 296/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8652\n",
      "Epoch 297/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3211 - accuracy: 0.8659\n",
      "Epoch 298/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3211 - accuracy: 0.8662\n",
      "Epoch 299/300\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3213 - accuracy: 0.8659\n",
      "Epoch 300/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8659\n"
     ]
    }
   ],
   "source": [
    "classifier_3 = classifier\n",
    "prueba_3 = classifier_3.fit(X_train, y_train, epochs=300, batch_size=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b6b4a8b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8675"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = classifier_3.predict(X_test)\n",
    "np.sum(y_test.reshape(1,-1) == np.rint(y_pred.reshape(1, -1)))/len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaba711",
   "metadata": {},
   "source": [
    "Obtenemos la misma precisi√≥n. Adem√°s, observamos la evoluci√≥n de la misma durante el entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "44960a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x257db281a30>]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(prueba_3.history['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8411a018",
   "metadata": {},
   "source": [
    "Se trata de un movimiento oscilante, incluso peri√≥dico. El modelo no es capaz de obtener valores mayores de precisi√≥n. Por lo tanto, no es un fallo de las √©pocas elegidas o el tama√±o de la tanda, sino de la arquitectura del modelo en s√≠. Puede que no haya suficientes unidades ocultas. \n",
    "\n",
    "Recogemos los resultados de las pruebas obtenidas en la siguiente tabla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e71bf81a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>0.8670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300</td>\n",
       "      <td>100</td>\n",
       "      <td>0.8675</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Epochs  Batch size  Accuracy\n",
       "0     100           1    0.8640\n",
       "1     100         100    0.8670\n",
       "2     300         100    0.8675"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = [0.864, 0.867, 0.8675]\n",
    "epochs = [100, 100, 300]\n",
    "batch_size = [1, 100, 100]\n",
    "data = { \"Epochs\" : epochs, \"Batch size\" : batch_size,\"Accuracy\" : acc}\n",
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fedef7",
   "metadata": {},
   "source": [
    "Vemos que los 3 casos nos dan el mismo resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60c0fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
